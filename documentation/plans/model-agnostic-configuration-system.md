# Model-Agnostic Configuration System for SOP Agent

## Overview

The SOP Agent now supports a model-agnostic configuration system that allows seamless switching between different LLM models (GPT-4, GPT-5, O3, etc.) without hardcoding model-specific parameter logic in the codebase.

## Problem Solved

Previously, the SOP agent was hardcoded to use GPT-4o with specific parameters like `temperature`. When trying to upgrade to GPT-5, several issues emerged:

1. **GPT-5 doesn't support `temperature`** - it uses `reasoning_effort` and `verbosity` instead
2. **Parameter validation** - Invalid parameters cause API errors
3. **Model-specific logic** - Code needed to know which parameters work with which models
4. **Configuration inflexibility** - No way to switch models without code changes

## Solution Architecture

### üèóÔ∏è **Core Components**

#### 1. `ModelConfig` Class
- **Purpose**: Unified configuration for any LLM model
- **Features**: Dynamic parameter handling, type conversion, validation
- **Key Methods**:
  - `to_langchain_kwargs()` - Converts config to LangChain parameters
  - `create_langchain_llm()` - Creates configured LLM instance
  - `from_dict()` - Creates config from dictionary/YAML

#### 2. `ModelConfigs` Utility Class
- **Purpose**: Predefined configurations for common scenarios
- **Includes**: GPT-4 defaults, GPT-5 defaults, environment-based config
- **Extensible**: Easy to add new model configurations

#### 3. `create_configured_llm()` Function
- **Purpose**: Convenience function for quick LLM creation
- **Features**: Supports overrides, multiple input formats, fallback logic

### üîß **Parameter Handling Strategy**

The system uses **dynamic parameter passing** through LangChain's `model_kwargs`:

```python
# GPT-4 Configuration
{
    "model": "gpt-4o",
    "max_tokens": 4000,
    "model_kwargs": {
        "temperature": 0.7  # GPT-4 specific
    }
}

# GPT-5 Configuration  
{
    "model": "gpt-5",
    "max_tokens": 4000,
    "model_kwargs": {
        "reasoning_effort": "medium",  # GPT-5 specific
        "verbosity": "medium"          # GPT-5 specific
    }
}
```

**Benefits**:
- ‚úÖ **No model-specific logic in code** - Configuration determines valid parameters
- ‚úÖ **OpenAI API handles validation** - Invalid parameters cause clear API errors
- ‚úÖ **Future-proof** - New models with new parameters work automatically
- ‚úÖ **Backward compatible** - Existing GPT-4 configurations continue working

## Usage Patterns

### üéØ **1. Predefined Configurations**

```python
from plexus.cli.experiment.model_config import ModelConfigs

# Use GPT-4 defaults
gpt4_llm = ModelConfigs.gpt_4o_default().create_langchain_llm()

# Use GPT-5 with high reasoning
gpt5_llm = ModelConfigs.gpt_5_high_reasoning().create_langchain_llm()

# Use environment-based configuration
env_llm = ModelConfigs.from_environment().create_langchain_llm()
```

### üéØ **2. Dictionary/YAML Configuration**

```python
# From dictionary
config_dict = {
    "model": "gpt-5",
    "reasoning_effort": "high",
    "verbosity": "medium",
    "max_tokens": 6000,
    "custom_parameter": "value"  # Goes to model_kwargs automatically
}
llm = ModelConfig.from_dict(config_dict).create_langchain_llm()

# From YAML (loaded externally)
yaml_config = """
model: gpt-5
reasoning_effort: high
verbosity: medium
max_tokens: 6000
"""
config = yaml.safe_load(yaml_config)
llm = ModelConfig.from_dict(config).create_langchain_llm()
```

### üéØ **3. Environment Variables**

```bash
# Set environment variables
export MODEL_NAME=gpt-5
export MODEL_REASONING_EFFORT=high
export MODEL_VERBOSITY=medium
export MODEL_MAX_TOKENS=6000
```

```python
# Use in code
llm = ModelConfigs.from_environment().create_langchain_llm()
```

### üéØ **4. SOP Agent Integration**

```python
# Pass model config to SOP agent
model_config = {
    "model": "gpt-5",
    "reasoning_effort": "high",
    "verbosity": "medium"
}

agent = StandardOperatingProcedureAgent(
    procedure_id="experiment_001",
    procedure_definition=procedure_def,
    mcp_server=mcp_server,
    model_config=model_config  # üéâ New parameter
)

# Agent automatically uses configured model for:
# - Manager guidance LLM
# - Worker coding assistant LLM  
# - Summarization LLM
# - Conversation filtering (token counting)
```

## Migration Guide

### üìà **From Hardcoded GPT-4**

**Before (Hardcoded)**:
```python
# In sop_agent_base.py
sop_guidance_llm = ChatOpenAI(
    model="gpt-4o",
    temperature=0.1,
    openai_api_key=self.openai_api_key,
    stream=False
)
```

**After (Configurable)**:
```python
# Configuration drives model selection
model_config = {
    "model": "gpt-4o",  # or "gpt-5"
    "temperature": 0.1   # or "reasoning_effort": "high"
}

# SOP agent handles LLM creation automatically
agent = StandardOperatingProcedureAgent(..., model_config=model_config)
```

### üìà **Upgrading to GPT-5**

**Step 1**: Update your configuration
```python
# OLD GPT-4 config
old_config = {
    "model": "gpt-4o",
    "temperature": 0.7,
    "max_tokens": 4000
}

# NEW GPT-5 config
new_config = {
    "model": "gpt-5",
    "reasoning_effort": "medium",
    "verbosity": "medium", 
    "max_tokens": 4000
}
```

**Step 2**: No code changes needed!
```python
# Same code works with both configs
agent = StandardOperatingProcedureAgent(..., model_config=new_config)
```

## Configuration Examples

### üîß **GPT-4 Configurations**

```python
# Standard experimentation
gpt4_standard = {
    "model": "gpt-4o",
    "temperature": 0.7,
    "max_tokens": 4000
}

# Precise analysis
gpt4_precise = {
    "model": "gpt-4o", 
    "temperature": 0.1,
    "max_tokens": 6000
}
```

### üîß **GPT-5 Configurations**

```python
# Balanced reasoning
gpt5_balanced = {
    "model": "gpt-5",
    "reasoning_effort": "medium",
    "verbosity": "medium",
    "max_tokens": 4000
}

# High-powered analysis
gpt5_analysis = {
    "model": "gpt-5",
    "reasoning_effort": "high",
    "verbosity": "high",
    "max_tokens": 6000
}

# Fast responses
gpt5_fast = {
    "model": "gpt-5-mini",
    "reasoning_effort": "low",
    "verbosity": "low",
    "max_tokens": 2000
}
```

### üîß **Future Model Support**

```python
# Example O3 configuration (hypothetical)
o3_config = {
    "model": "o3",
    "inference_mode": "balanced",
    "context_length": "extended",
    "max_tokens": 8000
}

# The system will automatically pass these to model_kwargs
# No code changes needed to support new models!
```

## Role-Specific Configurations

The SOP agent uses different LLM instances for different roles:

### üéØ **Manager Agent (SOP Guidance)**
- **Purpose**: Provides coaching and procedural guidance
- **Optimization**: Precise, consistent, concise
- **GPT-4**: Low temperature (0.1)
- **GPT-5**: High reasoning effort, low verbosity

### üéØ **Worker Agent (Coding Assistant)**  
- **Purpose**: Problem solving, tool usage, analysis
- **Optimization**: Creative, detailed, explanatory
- **GPT-4**: Moderate temperature (0.7)
- **GPT-5**: Medium reasoning effort, high verbosity

### üéØ **Summarization**
- **Purpose**: Final summaries and reports
- **Optimization**: Balanced, clear, comprehensive
- **GPT-4**: Balanced temperature (0.3)
- **GPT-5**: Low reasoning effort, medium verbosity

## Environmental Configuration

### üåç **Environment Variables**

Support for runtime configuration without code changes:

```bash
# Model selection
export MODEL_NAME=gpt-5

# GPT-4 parameters
export MODEL_TEMPERATURE=0.7
export MODEL_MAX_TOKENS=4000

# GPT-5 parameters  
export MODEL_REASONING_EFFORT=high
export MODEL_VERBOSITY=medium

# Custom parameters
export MODEL_CUSTOM_PARAM=custom_value
```

### üåç **Usage in Code**

```python
# Automatically picks up environment configuration
config = ModelConfigs.from_environment()
llm = config.create_langchain_llm()
```

## Testing and Validation

### ‚úÖ **Parameter Validation Strategy**

**Philosophy**: Let the OpenAI API handle validation rather than hardcoding model-specific logic.

**Benefits**:
- Always up-to-date with latest API changes
- No maintenance burden for parameter compatibility
- Clear error messages from OpenAI when parameters are invalid
- Future-proof for new models and parameters

**Example**:
```python
# Invalid parameter combination
config = {
    "model": "gpt-5",
    "temperature": 0.7  # Invalid for GPT-5
}

# Error occurs at LLM creation time with clear OpenAI error message
try:
    llm = ModelConfig.from_dict(config).create_langchain_llm()
except Exception as e:
    print(f"Configuration error: {e}")
    # OpenAI API provides clear error about invalid parameter
```

### ‚úÖ **Testing Strategy**

1. **Unit Tests**: Test configuration parsing and parameter building
2. **Integration Tests**: Test with different model configurations  
3. **Live Tests**: Verify actual API compatibility (when available)

## Benefits Summary

### üéâ **For Developers**
- ‚úÖ **No model-specific code** - Configuration handles differences
- ‚úÖ **Easy model switching** - Change config, not code
- ‚úÖ **Future-proof** - New models work automatically
- ‚úÖ **Backward compatible** - Existing code continues working

### üéâ **For Configuration**
- ‚úÖ **Flexible parameter passing** - Any parameter can be configured
- ‚úÖ **Multiple config sources** - Dict, YAML, environment variables
- ‚úÖ **Runtime configuration** - No redeployment needed for model changes
- ‚úÖ **Clear error handling** - OpenAI API provides validation

### üéâ **For Operations**
- ‚úÖ **Model cost optimization** - Easy switching between model sizes
- ‚úÖ **Performance tuning** - Adjust parameters per use case
- ‚úÖ **Environment-specific configs** - Dev/staging/prod differences
- ‚úÖ **A/B testing** - Compare models with same codebase

## Implementation Status

‚úÖ **Completed**:
- Model configuration system (`model_config.py`)
- SOP agent integration (`sop_agent_base.py`)
- Predefined configurations and examples
- Environment variable support
- Comprehensive documentation
- Working test validation

üéØ **Ready for Use**:
- Drop-in replacement for existing GPT-4 usage
- Immediate GPT-5 support when available
- Extensible for future models (O3, Claude, etc.)

## Next Steps

1. **Test with actual GPT-5 API** when available
2. **Add configuration validation** if desired (optional)
3. **Create YAML config files** for different environments
4. **Extend to other model providers** (Claude, etc.) using same pattern

This system provides a clean, flexible foundation for model-agnostic LLM usage in the SOP agent while preserving all existing functionality and enabling easy upgrades to newer models! üöÄ
