# Plexus Reports Feature Plan

**Status Legend:**
*   â¬œ Not Started / To Do
*   ðŸŸ¡ In Progress
*   âœ… Completed

***Note on Testing:*** *Test files in this project are typically located directly adjacent to the source code file they are testing (e.g., `service.py` would have a corresponding `service_test.py` in the same directory).*

## Introduction

This document outlines the plan for implementing a flexible and extensible reporting system within the Plexus platform. The goal is to provide a standardized way to define, generate, store, and view various types of reports and analyses without requiring bespoke dashboard pages or API schema changes for each new report type. This system will support reports like feedback analysis, topic modeling, score performance summaries, and more.

**Crucially, report generation will leverage the existing `Task` and `TaskStage` system for standardized status and progress tracking, ensuring consistency with other background jobs like Evaluations.**

**Note on Running CLI Commands:** When working within this specific project checkout (maybe `Plexus`, maybe `Plexus_2`, there are multiple clones), run CLI commands using `python -m plexus.cli.CommandLineInterface [command] [args...]` from the project root directory. This ensures the local code is executed without interfering with the globally installed `plexus` module from a different repository.

## Core Concepts

The reporting system will be built around **four** core concepts:

*   **`ReportConfiguration`**: Defines the structure, content sources, and parameters for a specific type of report using Markdown and Jinja2 templating. It acts as a template for generating reports and specifies which `ReportBlock` Python classes to execute.
*   **`Report`**: Represents a specific instance of a report generated based on a `ReportConfiguration`. It stores the final rendered output (e.g., Markdown) in its `output` field and links to the individual `ReportBlock` results.
*   **`ReportBlock` (Database Model)**: Stores the structured JSON output and optional logs generated by a specific Python `ReportBlock` execution within a `Report` run. Each block has a defined `position` and an optional `name`.
*   **`ReportBlock` (Python Class)**: Reusable Python components responsible for generating specific sections or data points within a report. These blocks encapsulate the logic for fetching data and performing analysis, returning JSON data to be stored in the `ReportBlock` database model.
*   **`Task` / `TaskStage`:** The standard mechanism for dispatching, monitoring, and tracking the progress of the report generation job itself. Each `Report` instance will be directly linked to a corresponding `Task` record.

## Data Models

### `ReportConfiguration`

*   **Storage:** Likely stored as YAML or JSON within a new database model (e.g., `ReportConfiguration`). This allows for versioning and easy editing.
*   **Structure:**
    *   `name`: Human-readable name for the configuration.
    *   `description`: Optional description.
    *   `accountId`: Link to the owning account.
    *   `configuration`: The core YAML/JSON definition. This would specify:
        *   Static content (headers, paragraphs, images).
        *   Report Blocks to include, along with their specific parameters (e.g., `scorecardId`, `timeRange`, `pythonClass`).
        *   Layout or ordering information for the blocks.
    *   Standard metadata (`createdAt`, `updatedAt`, etc.).
*   **API Access:** Use `plexus.dashboard.api.models.report_configuration.ReportConfiguration` model class.

### `Report`

*   **Storage:** A new database model (`Report`) linked to a `ReportConfiguration` and a `Task`.
*   **Structure:**
    *   `reportConfigurationId`: Link to the configuration used.
    *   `accountId`: Link to the owning account.
    *   `name`: Can be auto-generated or user-defined.
    *   `taskId`: **Required** link to the associated `Task` record that handles the generation process and status tracking.
    *   `createdAt`, `updatedAt`: Standard metadata.
    *   `parameters`: Parameters used for this specific run (might override or supplement configuration).
    *   `output`: The final rendered report output, stored as a string (e.g., Markdown). Generated by processing the `ReportConfiguration.configuration` template.
    *   `reportBlocks`: A one-to-many relationship linking to the individual `ReportBlock` results generated for this report.
    *   `shareLinks`: Association for shareable URLs.
    *   **(Removed Fields):** `status`, `startedAt`, `completedAt`, `errorMessage`, `errorDetails` are **removed** from this model. This information is now managed by the associated `Task` record.
*   **API Access:** Use `plexus.dashboard.api.models.report.Report` model class.

### `ReportBlock`

*   **Storage:** A new database model (`ReportBlock`) linked to a `Report`.
*   **Structure:**
    *   `reportId`: Link to the parent `Report`.
    *   `name`: Optional user-defined name for the block (extracted from the block definition in the configuration).
    *   `position`: Required integer indicating the order/position of the block within the report configuration.
    *   `output`: The structured data generated by the corresponding Python `ReportBlock` class, stored as JSON.
    *   `log`: Optional string containing logs or messages from the block's execution.
    *   Standard metadata (`createdAt`, `updatedAt`).
*   **Indexes:**
    *   `byReportAndName`: GSI to query blocks by `reportId` and `name`.
    *   `byReportAndPosition`: GSI to query blocks by `reportId` and `position`.
*   **API Access:** Use `plexus.dashboard.api.models.report_block.ReportBlock` model class.

## Backend Implementation

*(Note: The API client is available via `plexus.dashboard.api.client.PlexusDashboardClient`)*

### Python `ReportBlock` Framework

*   Define a base Python class (e.g., `plexus.reports.blocks.BaseReportBlock`).
*   Subclasses will implement specific report generation logic (e.g., `FeedbackAnalysisBlock`, `TopicModelBlock`, `ScorePerformanceBlock`).
*   Blocks will implement a standard method (e.g., `generate(config, params)`) that returns a JSON-serializable dictionary (stored in `ReportBlock.output`) and optionally a log string (stored in `ReportBlock.log`).
*   Blocks should have access to Plexus data fetching utilities (e.g., to query `Score`, `Evaluation`, `Item` data via the API or direct DB access if necessary).

### Report Generation Service

*   A mechanism to trigger report generation based on a `ReportConfiguration`, **which will always create and dispatch a `Task`**. The service logic itself will be executed via a Celery task triggered by this `Task` record.
*   **Invocation:** The service will be invoked with a `task_id`.
*   The service, running within the Celery task context, will:
    1.  Fetch the associated `Task` record using the `task_id`.
    2.  Load the `ReportConfiguration` (referenced by the Task or Report) and run `parameters` (likely stored in the Task or Report).
    3.  **Update Task Status:** Set the `Task` status to `RUNNING` and initialize progress tracking (e.g., using `TaskProgressTracker`).
    4.  **Create Report Record:** Create the `Report` database record, linking it to the `Task` (`taskId`) and `ReportConfiguration`.
    5.  Parse the `ReportConfiguration.configuration` Markdown to extract block definitions and the original Markdown template.
    6.  **Process Blocks:** For each extracted block definition: Instantiate and call the `generate` method. Create `ReportBlock` records storing the results. **Update Task/Stage progress via `TaskProgressTracker` as blocks complete.**
    7.  **Store Original Markdown:** Store the reconstructed, original Markdown string (from step 5) into the `Report.output` field.
    8.  **Update Final Task Status:** Upon successful completion or failure, update the associated `Task` record's final `status`, `completedAt`, `errorMessage`, `errorDetails` etc. **The `Report` record itself is not directly updated with status information.**

## Frontend Implementation (Dashboard)

### Management Interface

*   New dashboard section for "Reports".
*   View/List existing `ReportConfiguration`s and `Report`s.
*   Create/Edit `ReportConfiguration`s:
    *   Potentially a YAML/JSON editor.
    *   A more user-friendly UI builder could be a future enhancement.
*   Trigger new `Report` runs from a configuration.

### Report Viewing

*   Dedicated page or component to display a `Report`.
*   Fetch the `Report` record, including its `output` string and its associated `ReportBlock` records (sorted by `position`).
*   Render the `Report.output` string, likely using a Markdown renderer component.
*   Display the data from the associated `ReportBlock` records. This could involve:
    *   A separate section/tab listing each block (by `name` or `position`).
    *   Dynamically rendering the `output` JSON from each `ReportBlock` using appropriate React components (tables, charts, key metrics, text sections) based on the JSON structure or hints within it.
*   **Sharing:** Integrate with the existing `ShareLink` system to allow sharing report URLs.
*   **Printing:** Implement CSS media queries (`@media print`) to provide a clean, printable version of the report view, removing UI chrome.

## Implementation Plan & Checklist

*   âœ… **Define Models:** Define `ReportConfiguration`, `Report`, and `ReportBlock` models in `dashboard/amplify/data/resource.ts`.
    *   âœ… Add fields for `ReportConfiguration` (name, description, accountId, configuration (json), createdAt, updatedAt).
    *   âœ… **Modify `Report`:** Add required `taskId` field. **Remove** `status`, `startedAt`, `completedAt`, `errorMessage`, `errorDetails` fields.
    *   âœ… Add fields for `ReportBlock` (reportId, name, position, output (json), log, createdAt, updatedAt).
*   âœ… **Define Relationships:**
    *   âœ… Add necessary relationships (`Account` -> `ReportConfiguration`, `ReportConfiguration` -> `Report`, `Account` -> `Report`).
    *   âœ… **Add `Task <-> Report` relationship:** Add `report: hasOne` to `Task` and `task: belongsTo` (linked via required `taskId`) to `Report`.
    *   âœ… Add (`Report` -> `ReportBlock`).
*   âœ… **Add Indexes:**
    *   âœ… Define required secondary indexes (`ReportConfiguration` by accountId/updatedAt, name; `ReportBlock` by reportId/name, reportId/position).
    *   âœ… **Modify `Report` Indexes:** Remove index on `status`. Add index on `taskId`.

### Phase 1: Backend Foundation (Post-Schema)

*   âœ… **Create Base Python Class:** Create the base `plexus.reports.blocks.BaseReportBlock` Python class (`plexus/reports/blocks/base.py`) with a placeholder `generate` method.
*   âœ… **Verify Phase 1:** Confirm models exist in the backend and that the auto-generated base GraphQL CRUD operations (e.g., `getReport`, `listReports`, `createReportConfiguration`) work as expected via AppSync console or tests.

### Phase 2: Report Generation (Service & Triggering)

*   âœ… **Use Existing Test Block:** Use the existing `ScoreInfo` block (in `plexus/reports/blocks/score_info.py`) for initial testing instead of creating a separate `HelloWorld` block. *(Renamed from ScoreInfoBlock)*
*   âœ… **Develop Generation Service Core:** Create Python service logic (`plexus.reports.service`) that:
    *   âœ… Takes a `task_id` as input.
    *   âœ… Fetches the associated `Task` and loads `ReportConfiguration` and parameters.
    *   âœ… **Integrate Task Progress:** Use `TaskProgressTracker` to update `Task` and `TaskStage` status (e.g., `RUNNING`, progress updates during block processing, `COMPLETED`/`FAILED`).
    *   âœ… Creates the `Report` record linked to the `Task`.
    *   âœ… Parses the `configuration` field (Markdown) to identify block definitions.
    *   âœ… Processes Blocks: Instantiates and calls `generate` for each block. Creates `ReportBlock` records.
    *   âœ… Stores the original Markdown template in `Report.output`.
*   âœ… **Implement CLI Trigger:** Create the `plexus report run --config <config_identifier> [params...]` CLI command that:
    *   âœ… Parses arguments.
    *   âœ… **Creates a `Task` record** for the report generation.
    *   âœ… Dispatches the Celery task (`generate_report_task`) using the created `task_id`.
*   âœ… **(Removed) Basic Status Updates:** Status updates are now handled via the `Task` model and `TaskProgressTracker`.
*   âœ… **Implement Celery Task (`generate_report_task`):**
    *   âœ… Takes `task_id`.
    *   âœ… Calls the `plexus.reports.service.generate_report` service function, passing the `task_id`.
    *   âœ… **Handles top-level exceptions:** Catches errors from the service call and updates the corresponding `Task` record to `FAILED` with error details.
*   âœ… **Implement Celery Dispatch Mechanism:** The `plexus report run` command handles Task creation and Celery dispatch.
*   âœ… **(Modified) Add Error Handling:** Error handling in the service updates the associated `Task` record. Celery task handles errors *calling* the service.
*   ðŸŸ¡ **Verify Phase 2:** Confirm reports can be generated via CLI/Celery, data is stored correctly in `Report` and `ReportBlock`, and **Task/TaskStage status updates correctly**.
    *   *Status:* We have successfully created a test `ReportConfiguration` (`"Test ScoreInfo Report"`, ID: `f496664a-82ee-404d-a266-e3dc871a13b9`) using `python -m plexus.cli.CommandLineInterface report create-config --name 'Test ScoreInfo Report' --scorecard cmg_edu_v1_0 --score Greeting`. We then triggered the generation using `python -m plexus.cli.CommandLineInterface report run --config 'Test ScoreInfo Report'`, which created Task `4b688330-704e-485c-b7ca-8e0d95a16346`. The task is currently `PENDING`/`QUEUED`.
    *   ***NEXT:*** *Start a Celery worker (`python -m plexus.cli.CommandLineInterface command worker`) and observe its logs to confirm it processes Task `4b688330-704e-485c-b7ca-8e0d95a16346` and updates its status/stages correctly. Check final status using `python -m plexus.cli.CommandLineInterface tasks info --id 4b688330-704e-485c-b7ca-8e0d95a16346`.*

### Phase 3: CLI Inspection Tools (Pre-UI Validation)

*   **Note on ID/Name Lookup:** Commands accepting `<id_or_name>` should intelligently attempt lookup: Check if input looks like a UUID. If yes, try ID first, then name. If no, try name first, then ID. Always try both before failing.
*   âœ… **Implement `plexus report config list`:** Create a CLI command to list `ReportConfiguration` records. Use `rich` for formatted table output.
*   âœ… **Implement `plexus report config show <id_or_name>`:** Create a CLI command to display details of a specific `ReportConfiguration`. Use `rich` panels/syntax highlighting. Implement ID/Name lookup.
*   âœ… **Implement `plexus report list [--config <id_or_name>]`:** Create a CLI command to list `Report` records, including linked `taskId` and basic `Task` status. Use `rich` table output. Support optional filtering by `ReportConfiguration` (implementing ID/Name lookup for the filter value).
*   âœ… **Implement `plexus report show <id_or_name>`:** Create a CLI command to display details of a specific `Report`, including parameters, output (rendered Markdown if possible), and associated `ReportBlock` summaries. Use `rich` panels. Implement ID/Name lookup.
*   âœ… **Implement `plexus report last`:** Create a CLI command to show the details of the most recently created `Report` (equivalent to `plexus report show` for the latest report). Use `rich` panels.
*   âœ… **Implement `plexus report block list <report_id>`:** Create a CLI command to list `ReportBlock` records associated with a specific `Report`. Use `rich` for formatted table output. *Note: `<report_id>` here should strictly be the ID.* 
*   âœ… **Implement `plexus report block show <report_id> <block_identifier>`:** Create a CLI command to display the details (name, position, output JSON, log) of a specific `ReportBlock` (identified by position or name). Use `rich` panels/syntax highlighting. *Note: `<report_id>` here should strictly be the ID.* 
*   âœ… **Verify Phase 3:** Confirm these CLI commands function correctly, including filtering and ID/Name lookup, providing the necessary visibility into report data.

### Phase 4: Backend & CLI Testing

*   â¬œ **Objective:** Verify the end-to-end functionality of report configuration management, generation triggering via CLI, Celery task execution, data storage, status tracking, and CLI inspection tools.
*   â¬œ **Prerequisites:**
    *   Ensure a Celery worker can be started (`python -m plexus.cli.CommandLineInterface command worker`).
    *   Ensure necessary environment variables are set (e.g., `PLEXUS_ACCOUNT_KEY` in `.env`) and **loaded by the application**.
    *   Ensure the database schema is up-to-date.
*   â¬œ **Test Steps:**
    1.  âœ… **`config list`:**
        *   Run `python -m plexus.cli.CommandLineInterface report config list`. Verify existing configs are listed correctly for the default account.
        *   Run `python -m plexus.cli.CommandLineInterface report config list --account call-criteria`. Verify filtering works.
        *   Run `python -m plexus.cli.CommandLineInterface report config list --account <invalid_key/id>`. Verify appropriate error/empty message.
    2.  â¬œ **`config create`:**
        *   âœ… Run `python -m plexus.cli.CommandLineInterface report config create --name "CLI Test Config" --block-class ScoreInfo --block-param scorecard=cmg_edu_v1_0 --block-param score=Greeting`. Verify successful creation message (`Successfully created Report Configuration...`) and that the config appears in `config list` output.
        *   â¬œ Attempt creation with missing required options (e.g., `--name`). Verify Click error. (`python -m plexus.cli.CommandLineInterface report config create --block-class ScoreInfo ...`)
        *   â¬œ Attempt creation with invalid block params. Verify error message. (`python -m plexus.cli.CommandLineInterface report config create --name "Invalid Param Test" --block-class ScoreInfo --block-param invalid-param`)
    3.  â¬œ **`config show`:**
        *   Run `python -m plexus.cli.CommandLineInterface report config show "CLI Test Config"`. Verify details are displayed correctly using ID/Name lookup, including the JSON/Markdown configuration content with syntax highlighting.
        *   Run `python -m plexus.cli.CommandLineInterface report config show <valid_config_id>`. Verify ID lookup works.
        *   Run `python -m plexus.cli.CommandLineInterface report config show "NonExistent Config"`. Verify "not found" message.
    4.  â¬œ **`report run` (Core Test):**
        *   Run `python -m plexus.cli.CommandLineInterface report run --config "CLI Test Config"`. Verify:
            *   Task creation message with Task ID is shown.
            *   Task dispatch message is shown.
        *   Run `python -m plexus.cli.CommandLineInterface report run --config <invalid_config_id_or_name>`. Verify error message (config resolution failure).
        *   Run `python -m plexus.cli.CommandLineInterface report run --config "CLI Test Config" invalid_param=test`. Verify parameter parsing error or successful run with parameters logged in Task metadata.
        *   **Monitor Celery Worker:** Start worker (`python -m plexus.cli.CommandLineInterface command worker`). Observe logs for:
            *   Task reception (`generate_report_task` started with correct `task_id`).
            *   Service execution (`plexus.reports.service` logs).
            *   `Report` record creation log/message.
            *   `ReportBlock` record creation log/message.
            *   Task/Stage progress updates via `TaskProgressTracker`.
            *   Successful completion or specific error logging.
        *   **Check Task Status:** Use `python -m plexus.cli.CommandLineInterface task get --id <task_id_from_run>` to check final status (`COMPLETED` or `FAILED`), completion time, error messages (if any).
        *   **Check Database:** (Optional) Manually inspect `Report` and `ReportBlock` tables to confirm data persistence.
    5.  â¬œ **`report list`:**
        *   Run `python -m plexus.cli.CommandLineInterface report list`. Verify the newly generated report appears with correct Config ID, Task ID, and fetched Task Status.
        *   Run `python -m plexus.cli.CommandLineInterface report list --config "CLI Test Config"`. Verify filtering works (using ID/Name lookup for the config).
        *   Run `python -m plexus.cli.CommandLineInterface report list --config "NonExistent Config"`. Verify appropriate "not found" or empty message.
    6.  â¬œ **`report show`:**
        *   Identify the ID or Name of the report generated in step 4.
        *   Run `python -m plexus.cli.CommandLineInterface report show <report_id_or_name>`. Verify:
            *   Correct Report details are shown.
            *   Associated Task details (Status, Timestamps, Errors, Metadata) are fetched and displayed.
            *   Report Output (Markdown template) is displayed.
            *   Associated Report Blocks summary table is shown.
            *   ID/Name lookup works for the report itself.
        *   Run `python -m plexus.cli.CommandLineInterface report show <non_existent_report_id>`. Verify "not found" message.
    7.  â¬œ **`report last`:**
        *   Run `python -m plexus.cli.CommandLineInterface report last`. Verify it finds the most recently generated report (from step 4) and displays the same details as `report show` for that report.
    8.  â¬œ **`block list`:**
        *   Get the `report_id` from step 4.
        *   Run `python -m plexus.cli.CommandLineInterface report block list <report_id>`. Verify the blocks created by the `ScoreInfo` class appear with correct position, name (if any), output summary, and log status.
        *   Run `python -m plexus.cli.CommandLineInterface report block list <invalid_report_id>`. Verify "not found" or empty message.
    9.  â¬œ **`block show`:**
        *   Get the `report_id` and a `block_identifier` (position '0' or name if defined by the block) from the previous step.
        *   Run `python -m plexus.cli.CommandLineInterface report block show <report_id> <block_identifier>`. Verify:
            *   Correct block details (Position, Name, Timestamps) are shown.
            *   Output JSON is fetched, parsed, and displayed with syntax highlighting.
            *   Log content is fetched and displayed.
            *   Lookup works by both position (e.g., '0') and name (if applicable).
        *   Run `python -m plexus.cli.CommandLineInterface report block show <report_id> <invalid_identifier>`. Verify "not found" message.
*   â¬œ **Verify Phase 4:** Confirm all test steps pass, demonstrating reliable backend processing and CLI interaction for the reports feature.

### Phase 5: Frontend Basics (Management & Display)

*   â¬œ **Create "Reports" Dashboard Section:** Add a new top-level section/route (e.g., `/reports`) in the Next.js dashboard.
*   â¬œ **List Configurations:** Implement a UI table/list to display existing `ReportConfiguration`s fetched via GraphQL.
*   â¬œ **List Reports:** Implement a UI table/list to display existing `Report`s. **Fetch the associated `Task` record to display the generation status.**
*   â¬œ **Basic Configuration Editor:** Create a simple form/modal to create/edit `ReportConfiguration`s.
*   â¬œ **Trigger Generation from UI:** Add a button on the `ReportConfiguration` list/view to trigger a new report run **(invoking the Task creation/Celery dispatch mechanism from Phase 2)**.
*   â¬œ **Basic Report View:** Create a dedicated route/page (e.g., `/reports/[reportId]`).
*   â¬œ **Fetch Report Data:** Implement logic on the report view page to fetch the `Report` record (including `output`), its associated `ReportBlock` records, **and the associated `Task` record (for status/metadata).**
*   â¬œ **Initial Dynamic Rendering:** Develop a Markdown renderer for `Report.output`. Implement basic display for `ReportBlock` data. **Display generation status/errors fetched from the linked `Task`.**
*   â¬œ **Verify Phase 4:** Confirm basic UI for listing, creating configurations, triggering runs, and viewing simple reports works. **Verify status display reflects the linked Task.**

### Phase 6: Advanced Features & Polish

*   â¬œ **Implement Core Report Blocks:**
    *   â¬œ Implement `FeedbackAnalysisBlock`.
    *   â¬œ Implement `ScorePerformanceBlock`.
    *   â¬œ Implement `TopicModelBlock` (if applicable).
*   â¬œ **Develop Corresponding React Components:** Create specific React components to visualize the data from `ReportBlock.output` JSON (e.g., charts for performance, tables for feedback, topic lists).
*   â¬œ **Enhance Dynamic Rendering:** Improve the report viewing component to intelligently select and render the appropriate React component based on the structure/type information within the `ReportBlock.output` JSON.
*   â¬œ **Integrate Sharing:** Connect the `Report` model to the `ShareLink` system.
*   â¬œ **Improve Configuration Editor:** Consider a more user-friendly UI beyond raw YAML/JSON/Markdown editing (future enhancement).
*   â¬œ **Refine Print Styles:** Ensure the `@media print` styles produce a high-quality printed report, handling block rendering appropriately.
*   â¬œ **Verify Phase 5:** Test complex reports with various blocks, ensure proper visualization, sharing, and printing.

## Example Report Configuration

Here is an example of the content stored in the `ReportConfiguration.configuration` field:

```block name="Term Life - Temperature Check - Score Information"
class: ScoreInformation
scorecard: termlifev1
score: Temperature Check
```

The parameters for blocks work in the same way as parameters to the CLI tools: `scorecard` can be an ID, an external ID, a key, or a name. Same for `score`.

When the report is generated, each report block in the report will generate structured JSON output that will be stored in a separate `ReportBlock` entry, linked to the `Report`. The JSON output for each block goes into `ReportBlock.output`. The block's execution logs can be stored in `ReportBlock.log`.

*(Note: The example above shows `ScoreInformation`. We should ensure consistency with the actual class name, which we intend to be `ScoreInfo`.)*

