# Plexus Reports Feature Plan

**Status Legend:**
*   â¬œ Not Started / To Do
*   ðŸŸ¡ In Progress
*   âœ… Completed

***Note on Testing:*** *Test files in this project are typically located directly adjacent to the source code file they are testing (e.g., `service.py` would have a corresponding `service_test.py` in the same directory).*

---
**CRITICAL INSTRUCTIONS FOR AI ASSISTANTS WORKING ON THIS FEATURE:**

1.  **DO NOT RUN `plexus` COMMANDS DIRECTLY.** This project often exists alongside other clones. The globally installed `plexus` command likely points to a different repository. **ALWAYS** run CLI commands using the local Python module structure from the project root:
    ```bash
    python -m plexus.cli.CommandLineInterface [command] [args...]
    ```
    This ensures you are executing the code currently being worked on.

2.  **NEVER ASSUME A COMMAND WORKED BECAUSE IT PRODUCED NO OUTPUT.** This is a dangerous and incorrect assumption. Terminal commands, especially in this project, may succeed silently or fail silently (or with errors that are missed if you don't wait). **YOU MUST ALWAYS WAIT FOR THE COMMAND TO COMPLETE AND CAREFULLY ANALYZE ITS ACTUAL OUTPUT AND EXIT CODE.** If a command appears to produce no output, verify its success through subsequent commands (e.g., listing created items, checking status) before proceeding. Failure to do this will lead to incorrect actions and wasted effort.
---
**ðŸš¨ CRITICAL NOTE FOR TOMORROW (or next work session) ðŸš¨**

âœ… **Implement `ReportBlock.type` Handling:** The `type` field has been successfully implemented in both the backend and frontend. The backend now properly sets the `type` field when creating `ReportBlock` records, and the frontend uses this field directly for component selection instead of parsing the output JSON.

## Introduction

This document outlines the plan for implementing a flexible and extensible reporting system within the Plexus platform. The goal is to provide a standardized way to define, generate, store, and view various types of reports and analyses without requiring bespoke dashboard pages or API schema changes for each new report type. This system will support reports like feedback analysis, topic modeling, score performance summaries, and more.

**Crucially, report generation will leverage the existing `Task` and `TaskStage` system for standardized status and progress tracking, ensuring consistency with other background jobs like Evaluations.**

**Note on Running CLI Commands:** When working within this specific project checkout (maybe `Plexus`, maybe `Plexus_2`, there are multiple clones), run CLI commands using `python -m plexus.cli.CommandLineInterface [command] [args...]` from the project root directory. This ensures the local code is executed without interfering with the globally installed `plexus` module from a different repository.

## Core Concepts

The reporting system will be built around **four** core concepts:

*   **`ReportConfiguration`**: Defines the structure, content sources, and parameters for a specific type of report using Markdown and Jinja2 templating. It acts as a template for generating reports and specifies which `ReportBlock` Python classes to execute.
*   **`Report`**: Represents a specific instance of a report generated based on a `ReportConfiguration`. It stores the final rendered output (e.g., Markdown) in its `output` field and links to the individual `ReportBlock` results.
*   **`ReportBlock` (Database Model)**: Stores the structured JSON output and optional logs generated by a specific Python `ReportBlock` execution within a `Report` run. Each block has a defined `position` and an optional `name`.
*   **`ReportBlock` (Python Class)**: Reusable Python components responsible for generating specific sections or data points within a report. These blocks encapsulate the logic for fetching data and performing analysis, returning JSON data to be stored in the `ReportBlock` database model.
*   **`Task` / `TaskStage`:** The standard mechanism for dispatching, monitoring, and tracking the progress of the report generation job itself. Each `Report` instance will be directly linked to a corresponding `Task` record.

## Data Models

### `ReportConfiguration`

*   **Storage:** Likely stored as YAML or JSON within a new database model (e.g., `ReportConfiguration`). This allows for versioning and easy editing.
*   **Structure:**
    *   `name`: Human-readable name for the configuration.
    *   `description`: Optional description.
    *   `accountId`: Link to the owning account.
    *   `configuration`: The core YAML/JSON definition. This would specify:
        *   Static content (headers, paragraphs, images).
        *   Report Blocks to include, along with their specific parameters (e.g., `scorecardId`, `timeRange`, `pythonClass`).
        *   Layout or ordering information for the blocks.
    *   Standard metadata (`createdAt`, `updatedAt`, etc.).
*   **API Access:** Use `plexus.dashboard.api.models.report_configuration.ReportConfiguration` model class.

### `Report`

*   **Storage:** A new database model (`Report`) linked to a `ReportConfiguration` and a `Task`.
*   **Structure:**
    *   `reportConfigurationId`: Link to the configuration used.
    *   `accountId`: Link to the owning account.
    *   `name`: Can be auto-generated or user-defined.
    *   `taskId`: **Required** link to the associated `Task` record that handles the generation process and status tracking.
    *   `createdAt`, `updatedAt`: Standard metadata.
    *   `parameters`: Parameters used for this specific run (might override or supplement configuration).
    *   `output`: The final rendered report output, stored as a string (e.g., Markdown). Generated by processing the `ReportConfiguration.configuration` template.
    *   `reportBlocks`: A one-to-many relationship linking to the individual `ReportBlock` results generated for this report.
    *   `shareLinks`: Association for shareable URLs.
    *   **(Removed Fields):** `status`, `startedAt`, `completedAt`, `errorMessage`, `errorDetails` are **removed** from this model. This information is now managed by the associated `Task` record.
*   **API Access:** Use `plexus.dashboard.api.models.report.Report` model class.

### `ReportBlock`

*   **Storage:** A new database model (`ReportBlock`) linked to a `Report`.
*   **Structure:**
    *   `reportId`: Link to the parent `Report`.
    *   `name`: Optional user-defined name for the block (extracted from the block definition in the configuration).
    *   `position`: Required integer indicating the order/position of the block within the report configuration.
    *   `output`: The structured data generated by the corresponding Python `ReportBlock` class, stored as JSON.
    *   `log`: Optional string containing logs or messages from the block's execution.
    *   Standard metadata (`createdAt`, `updatedAt`).
*   **Indexes:**
    *   `byReportAndName`: GSI to query blocks by `reportId` and `name`.
    *   `byReportAndPosition`: GSI to query blocks by `reportId` and `position`.
*   **API Access:** Use `plexus.dashboard.api.models.report_block.ReportBlock` model class.

## Backend Implementation

*(Note: The API client is available via `plexus.dashboard.api.client.PlexusDashboardClient`)*

### Python `ReportBlock` Framework

*   Define a base Python class (e.g., `plexus.reports.blocks.BaseReportBlock`).
*   Subclasses will implement specific report generation logic (e.g., `FeedbackAnalysisBlock`, `TopicModelBlock`, `ScorePerformanceBlock`).
*   Blocks will implement a standard method (e.g., `generate(config, params)`) that returns a JSON-serializable dictionary (stored in `ReportBlock.output`) and optionally a log string (stored in `ReportBlock.log`).
*   Blocks should have access to Plexus data fetching utilities (e.g., to query `Score`, `Evaluation`, `Item` data via the API or direct DB access if necessary).

### Report Generation Service

*   A mechanism to trigger report generation based on a `ReportConfiguration`, **which will always create and dispatch a `Task`**. The service logic itself will be executed via a Celery task triggered by this `Task` record.
*   **Invocation:** The service will be invoked with a `task_id`.
*   The service, running within the Celery task context, will:
    1.  Fetch the associated `Task` record using the `task_id`.
    2.  Load the `ReportConfiguration` (referenced by the Task or Report) and run `parameters` (likely stored in the Task or Report).
    3.  **Update Task Status:** Set the `Task` status to `RUNNING` and initialize progress tracking (e.g., using `TaskProgressTracker`).
    4.  **Create Report Record:** Create the `Report` database record, linking it to the `Task` (`taskId`) and `ReportConfiguration`.
    5.  Parse the `ReportConfiguration.configuration` Markdown to extract block definitions and the original Markdown template.
    6.  **Process Blocks:** For each extracted block definition: Instantiate and call the `generate` method. Create `ReportBlock` records storing the results. **Update Task/Stage progress via `TaskProgressTracker` as blocks complete.**
    7.  **Store Original Markdown:** Store the reconstructed, original Markdown string (from step 5) into the `Report.output` field.
    8.  **Update Final Task Status:** Upon successful completion or failure, update the associated `Task` record's final `status`, `completedAt`, `errorMessage`, `errorDetails` etc. **The `Report` record itself is not directly updated with status information.**

### Synchronous vs. Asynchronous Execution (Clarification)

Initial testing and discussion revealed the need for two distinct report generation execution paths, while ensuring core logic reuse and consistent progress tracking:

1.  **Synchronous Execution (CLI):**
    *   The `plexus report run` CLI command **MUST** execute the report generation process *synchronously* within the same process.
    *   It **MUST** create a `Task` record to represent the job and use `TaskProgressTracker` to manage status and stages.
    *   It **MUST NOT** dispatch the job to Celery. It should directly invoke the core report generation logic.
    *   The CLI command will wait for the generation to complete and report the final `Task` status and the resulting `Report` ID.
    *   This path is primarily intended for direct user interaction, testing, and scenarios where immediate feedback is desired without relying on background workers.

2.  **Asynchronous Execution (via Celery Worker):** The core report generation logic is executed by a standard Celery worker (`plexus command worker`) processing the `generate_report_task`. This task can be dispatched in two primary ways:
    *   **a) Direct Celery Dispatch:** A user or script can directly dispatch the `generate_report_task` using `plexus command dispatch [task_id] [other_args...]`. The worker receives this directly via the Celery queue.
    *   **b) API/Lambda Trigger:**
        *   A `Task` record is created via the GraphQL API (e.g., initiated by the dashboard) with metadata indicating a report generation request (including the `report_configuration_id`).
        *   A separate process (e.g., an AWS Lambda function triggered by DynamoDB stream events on the `Task` table) detects the creation of this new `Task` record configured for report generation.
        *   This trigger process then dispatches the actual `generate_report_task` to the Celery queue, passing the `task_id`.
        *   A Celery worker then picks up this dispatched task and executes the generation.
    *   In both asynchronous scenarios, the Celery task handler (the Python function decorated with `@celery.task`, likely calling `generate_report(task_id)`) uses the `task_id` to initialize `TaskProgressTracker` and invokes the *same core report generation logic* as the synchronous path. This ensures consistency regardless of how the job was initiated.

**Implementation Note:** This requires refactoring `plexus.reports.service` to:

1.  **Isolate Core Logic:** Create a new private function, `_generate_report_core`, that encapsulates the primary report generation steps (loading config, creating `Report` record, parsing markdown, running blocks, creating `ReportBlock` records, updating progress, setting final status).
2.  **Define `_generate_report_core` Parameters:** This function will accept direct inputs:
    *   `report_config_id: str`
    *   `account_id: str`
    *   `run_parameters: dict`
    *   `client: PlexusDashboardClient`
    *   `tracker: TaskProgressTracker` (This is crucial for linking to the `Task` and updating progress/status).
    *   It should return the ID of the created `Report` record upon success, or raise an exception on failure (allowing the caller to handle final Task status updates via the tracker).
3.  **Adapt `generate_report(task_id)`:** Modify the existing function (used by Celery workers):
    *   It will retain its `task_id: str` signature.
    *   Fetch the `Task` record using the `task_id`.
    *   Extract `report_config_id`, `account_id`, and `run_parameters` from `Task.metadata`.
    *   Initialize the `TaskProgressTracker` using the `task_id`.
    *   Wrap the call to `_generate_report_core` in a `try...except` block.
    *   Call `_generate_report_core` with the extracted parameters and the tracker.
    *   On success, ensure the tracker marks the task as `COMPLETED`.
    *   On exception during the `_generate_report_core` call, use the tracker to mark the task as `FAILED` with the error details.
4.  **Adapt `plexus report run` CLI (Next Step):** This CLI command will be modified *after* the service refactoring to:
    *   Create a new `Task` record.
    *   Initialize `TaskProgressTracker`.
    *   Directly call `_generate_report_core` synchronously, passing the necessary parameters and the tracker.
    *   It will *not* involve Celery dispatch.

This refactoring ensures the core report generation logic is DRY and consistently uses the `TaskProgressTracker`, while supporting both synchronous CLI execution and asynchronous Celery-based execution.

## Frontend Implementation (Dashboard)

### Management Interface

*   New dashboard section for "Reports".
*   View/List existing `ReportConfiguration`s and `Report`s.
*   Create/Edit `ReportConfiguration`s:
    *   Potentially a YAML/JSON editor.
    *   A more user-friendly UI builder could be a future enhancement.
*   Trigger new `Report` runs from a configuration.

### Report Viewing

*   Dedicated page or component to display a `Report`.
*   Fetch the `Report` record, including its `output` string and its associated `ReportBlock` records (sorted by `position`).
*   Render the `Report.output` string, likely using a Markdown renderer component.
*   Display the data from the associated `ReportBlock` records. This could involve:
    *   A separate section/tab listing each block (by `name` or `position`).
    *   Dynamically rendering the `output` JSON from each `ReportBlock` using appropriate React components (tables, charts, key metrics, text sections) based on the JSON structure or hints within it.
*   **Sharing:** Integrate with the existing `ShareLink` system to allow sharing report URLs.
*   **Printing:** Implement CSS media queries (`@media print`) to provide a clean, printable version of the report view, removing UI chrome.

## Implementation Plan & Checklist

*   âœ… **Define Models:** Define `ReportConfiguration`, `Report`, and `ReportBlock` models in `dashboard/amplify/data/resource.ts`.
    *   âœ… Add fields for `ReportConfiguration` (name, description, accountId, configuration (json), createdAt, updatedAt).
    *   âœ… **Modify `Report`:** Add required `taskId` field. **Remove** `status`, `startedAt`, `completedAt`, `errorMessage`, `errorDetails` fields.
    *   âœ… Add fields for `ReportBlock` (reportId, name, position, output (json), log, createdAt, updatedAt).
*   âœ… **Define Relationships:**
    *   âœ… Add necessary relationships (`Account` -> `ReportConfiguration`, `ReportConfiguration` -> `Report`, `Account` -> `Report`).
    *   âœ… **Add `Task <-> Report` relationship:** Add `report: hasOne` to `Task` and `task: belongsTo` (linked via required `taskId`) to `Report`.
    *   âœ… Add (`Report` -> `ReportBlock`).
*   âœ… **Add Indexes:**
    *   âœ… Define required secondary indexes (`ReportConfiguration` by accountId/updatedAt, name; `ReportBlock` by reportId/name, reportId/position).
    *   âœ… **Modify `Report` Indexes:** Remove index on `status`. Add index on `taskId`.

### Phase 1: Backend Foundation (Post-Schema)

*   âœ… **Create Base Python Class:** Create the base `plexus.reports.blocks.BaseReportBlock` Python class (`plexus/reports/blocks/base.py`) with a placeholder `generate` method.
*   âœ… **Verify Phase 1:** Confirm models exist in the backend and that the auto-generated base GraphQL CRUD operations (e.g., `getReport`, `listReports`, `createReportConfiguration`) work as expected via AppSync console or tests.

### Phase 2: Report Generation (Service & Triggering)

*   âœ… **Use Existing Test Block:** Use the existing `ScoreInfo` block (in `plexus/reports/blocks/score_info.py`) for initial testing instead of creating a separate `HelloWorld` block. *(Renamed from ScoreInfoBlock)*
*   âœ… **Develop Generation Service Core:** Create Python service logic (`plexus.reports.service`) that:
    *   âœ… Takes a `task_id` as input.
    *   âœ… Fetches the associated `Task` and loads `ReportConfiguration` and parameters.
    *   âœ… **Integrate Task Progress:** Use `TaskProgressTracker` to update `Task` and `TaskStage` status (e.g., `RUNNING`, progress updates during block processing, `COMPLETED`/`FAILED`).
    *   âœ… Creates the `Report` record linked to the `Task`.
    *   âœ… Parses the `configuration` field (Markdown) to identify block definitions.
    *   âœ… Processes Blocks: Instantiates and calls `generate` for each block. Creates `ReportBlock` records.
    *   âœ… Stores the original Markdown template in `Report.output`.
*   âœ… **Implement CLI Trigger:** Create the `plexus report run --config <config_identifier> [params...]` CLI command that:
    *   âœ… Parses arguments.
    *   âœ… **Creates a `Task` record** for the report generation.
    *   âœ… **Initializes `TaskProgressTracker`** using the created `task_id`.
    *   âœ… **Directly calls the core generation logic** (e.g., `_generate_report_core`) synchronously.
    *   âœ… **Waits for completion** and reports final `Task` status and `Report` ID.
    *   âœ… **Does NOT dispatch to Celery.**
*   âœ… **(Removed) Basic Status Updates:** Status updates are now handled via the `Task` model and `TaskProgressTracker`.
*   âœ… **Implement Celery Task (`generate_report_task`):**
    *   âœ… Takes `task_id`.
    *   âœ… Calls the `plexus.reports.service.generate_report` service function, passing the `task_id`. (This function now wraps `_generate_report_core` and handles task loading/tracker init).
    *   âœ… **Handles top-level exceptions:** Catches errors from the service call and updates the corresponding `Task` record to `FAILED` with error details.
*   âœ… **(Removed/Clarified) Implement Celery Dispatch Mechanism:** Celery dispatch is now primarily handled by API/Lambda triggers based on `Task` creation, or manually via `plexus command dispatch`, *not* by the `plexus report run` command itself.
*   âœ… **Verify Phase 2:** Confirm reports can be generated via CLI, data is stored correctly in `Report` and `ReportBlock`, and **Task/TaskStage status updates correctly**. *(Synchronous CLI generation verified 2025-04-28. Celery path TBD)*.
    *   âœ… *Status:* We have successfully created test `ReportConfiguration`s via the CLI (`create-config` and `report config create --file ...`). We have also successfully triggered generation using `report run` via CLI, which creates Tasks and Reports. Task and Stage status updates work correctly for the synchronous path.
    *   â¬œ ***NEXT:*** *(Optional for now) Verify asynchronous generation via Celery worker if needed.*

### Phase 3: CLI Inspection Tools (Pre-UI Validation)

*   **CLI Output Style:** For consistency, prefer using `rich.panel.Panel` with `expand=True` for displaying multi-line details in CLI command outputs.
*   **Note on ID/Name Lookup:** Commands accepting `<id_or_name>` should intelligently attempt lookup: Check if input looks like a UUID. If yes, try ID first, then name. If no, try name first, then ID. Always try both before failing.
*   âœ… **Implement `plexus report config list`:** Create a CLI command to list `ReportConfiguration` records. Use `rich` for formatted table output. *(Verified)*
*   âœ… **Implement `plexus report config show <id_or_name>`:** Create a CLI command to display details of a specific `ReportConfiguration`. Use `rich` panels/syntax highlighting. Implement ID/Name lookup. *(Verified with nested panel)*
*   âœ… **Implement `plexus report list [--config <id_or_name>]`:** Create a CLI command to list `Report` records, including linked `taskId` and basic `Task` status. Use `rich` table output. Support optional filtering by `ReportConfiguration` (implementing ID/Name lookup for the filter value). *(Verified)*
*   âœ… **Implement `plexus report show <id_or_name>`:** Create a CLI command to display details of a specific `Report`, including parameters, output (raw markdown), and associated `ReportBlock` summaries. Use `rich` panels. Implement ID/Name lookup. *(Verified)*
*   âœ… **Implement `plexus report last`:** Create a CLI command to show the details of the most recently created `Report` (equivalent to `plexus report show` for the latest report). Use `rich` panels. *(Verified)*
*   âœ… **Verify Phase 3:** Confirm these CLI commands function correctly, including filtering and ID/Name lookup, providing the necessary visibility into report data. *(Verified)*

### Phase 4: Backend & CLI Testing

*   âœ… **Objective:** Verify the end-to-end functionality of report configuration management, generation triggering via CLI, Celery task execution, data storage, status tracking, and CLI inspection tools. *(Core CLI synchronous path verified)*
    *   â¬œ **Prerequisites:**
        *   Ensure a Celery worker can be started (`python -m plexus.cli.CommandLineInterface command worker`).
        *   Ensure necessary environment variables are set (e.g., `PLEXUS_ACCOUNT_KEY` in `.env`) and **loaded by the application**.
        *   Ensure the database schema is up-to-date.
        *   â¬œ **Create Test Config File:** Create a file named `test_config.md` with sample Markdown content, e.g.:
            ```markdown
            # Test Report Header

            This is a sample configuration.

            ```block name="Score Info Block"
            class: ScoreInfo
            scorecard: cmg_edu_v1_0
            score: Greeting
            ```
            ```
*   â¬œ **Test Steps:**
    1.  âœ… **`config list`:**
        *   Run `python -m plexus.cli.CommandLineInterface report config list`. Verify existing configs are listed correctly for the default account (resolved via `.env`).
    2.  âœ… **`config create`:** *(Manually Tested - See notes below)*
        *   âœ… Run `python -m plexus.cli.CommandLineInterface report config create --name "CLI Test Config" --file test_config.md`. Verify successful creation message (`Successfully created Report Configuration...`) and that the config appears in `config list` output. *(Verified manually)*
        *   âœ… **Update (2025-04-28):** Fixed `test_config.md` format (name inside YAML block). Re-ran create, **currently paused awaiting confirmation [y/N] to overwrite.**
        *   â¬œ Attempt creation with missing required options (e.g., `--name` or `--file`). Verify Click error. (`python -m plexus.cli.CommandLineInterface report config create --name "Missing File"`)
        *   â¬œ Attempt creation with a non-existent file path. Verify error message. (`python -m plexus.cli.CommandLineInterface report config create --name "Bad File Path" --file non_existent_file.md`)
    3.  âœ… **`config delete`:** *(Manually Tested - See notes below)*
        *   âœ… **Delete by Name (with prompt):** Run `python -m plexus.cli.CommandLineInterface report config delete "CLI Test Config"`. Verify:
            *   It finds the correct config (shows ID/Name).
            *   It prompts for confirmation (`Are you sure...?`).
            *   Respond 'y'. Verify success message.
            *   Run `config list` again and verify "CLI Test Config" is gone. *(Verified manually - Required multiple fixes 2025-04-28)*
        *   âœ… **Delete multiple by name (with prompt):** Run `python -m plexus.cli.CommandLineInterface report config delete "Example From File CLI"`. Verify it prompts for and successfully deletes multiple matching entries. *(Verified 2025-04-28)*
        *   âœ… **Delete multiple by name (skip prompt):** Run `python -m plexus.cli.CommandLineInterface report config delete "Example From File CLI" --yes`. *(Not directly tested with multiple, but tested on single entries below)*
        *   âœ… **Delete by Name (skip prompt):** Run `python -m plexus.cli.CommandLineInterface report config delete "CLI Test Config" --yes`. Verify:
            *   It finds the correct config.
            *   It prints the "Skipping confirmation" message. *(Verified 2025-04-28)*
            *   It prints the success message. *(Verified 2025-04-28)*
            *   Run `config list` again and verify the config is gone. *(Verified 2025-04-28)*
        *   âœ… **Delete Non-Existent:** Run `python -m plexus.cli.CommandLineInterface report config delete "NonExistent Config"`. Verify "not found" message. *(Verified 2025-04-28)*
        *   âœ… **Recreate final test config:** Run `python -m plexus.cli.CommandLineInterface report config create --name "CLI Test Config" --file test_config.md`. *(Verified 2025-04-28)*
    5.  âœ… **`report run` (Core Test):**
        *   Run `python -m plexus.cli.CommandLineInterface report run --config "CLI Test Config"`. Verify:
            *   Task creation message with Task ID is shown.
            *   Messages indicating synchronous execution and progress updates are shown (e.g., block processing logs, final status).
            *   **No Celery dispatch message is shown.**
            *   The command waits for completion before returning the prompt.
            *   Final `Task` status and `Report` ID are printed upon completion.
            *   âœ… **Verified (2025-04-28):** Command runs successfully after multiple fixes (missing methods, tracker errors, GraphQL issues, block parsing, async issues, model instantiation).
        *   â¬œ Run `python -m plexus.cli.CommandLineInterface report run --config <invalid_config_id_or_name>`. Verify error message (config resolution failure).
        *   â¬œ Run `python -m plexus.cli.CommandLineInterface report run --config "CLI Test Config" invalid_param=test`. Verify parameter parsing error or successful run with parameters logged in Task metadata.
        *   **(Removed) Monitor Celery Worker:** Worker monitoring is not needed for synchronous CLI execution testing. (Worker testing should happen separately for async paths).
        *   âœ… **Check Task Status:** Use `python -m plexus.cli.CommandLineInterface task get --id <task_id_from_run>` to check final status (`COMPLETED` or `FAILED`), completion time, error messages (if any). Verify this matches the status reported by the `report run` command directly. *(Verified status matches)*
        *   âœ… **Check Database:** (Optional) Manually inspect `Report` and `ReportBlock` tables to confirm data persistence. *(Verified Report and ReportBlock created)*
    6.  âœ… **`report list`:**
        *   Run `python -m plexus.cli.CommandLineInterface report list`. Verify the newly generated report appears with correct Config ID, Task ID, and fetched Task Status.
        *   Run `python -m plexus.cli.CommandLineInterface report list --config "CLI Test Config"`. Verify filtering works (using ID/Name lookup for the config).
        *   Run `python -m plexus.cli.CommandLineInterface report list --config "NonExistent Config"`. Verify appropriate "not found" or empty message.
        *   âœ… **Verified (2025-04-28):** Command works after fixing list handling in CLI command code.
        *   âœ… **Verified (2025-04-28 PM):** Refactored to use Panels, fixed alignment, fetches data only for displayed reports. Warnings for missing configs related to displayed reports are understood and expected.
        *   âœ… **Status:** Completed.
    7.  âœ… **`report show`:**
        *   Identify the ID or Name of the report generated in step 4.
        *   Run `python -m plexus.cli.CommandLineInterface report show <report_id_or_name>`. Verify:
            *   Correct Report details are shown.
            *   Associated Task details (Status, Timestamps, Errors, Metadata) are fetched and displayed.
            *   Report Output (Markdown template) is displayed.
            *   Associated Report Blocks summary table is shown.
            *   ID/Name lookup works for the report itself.
            *   âœ… **Verified (2025-04-28):** Basic command worked after fixing list handling for blocks.
        *   Run `python -m plexus.cli.CommandLineInterface report show <non_existent_report_id>`. Verify "not found" message.
        *   âœ… **Status:** Completed.
    8.  âœ… **`report last`:**
        *   Run `python -m plexus.cli.CommandLineInterface report last`. Verify it finds the most recently generated report (from step 4) and displays the same details as `report show` for that report.
        *   âœ… **Verified (2025-04-28):** Basic command worked after fixing sort parameter and list handling.
        *   âœ… **Status:** Completed.
*   âœ… **Verify Phase 4:** Confirm all test steps pass, demonstrating reliable backend processing and CLI interaction for the reports feature. *(Marking as complete as the core CLI synchronous path is functional and robust enough for now. Further testing/refinement can happen later if needed.)*
    *   **Notes (2025-04-28 Session End):**
        *   Successfully tested `report config` commands (list, show, create, delete).
        *   Successfully tested `report run` (sync), including improved error handling for block failures (class not found, internal errors).
        *   âœ… `report list`, `show`, and `last` commands work correctly.
        *   **Removed:** `report block` commands and tests.
        *   **CURRENT STATUS:** Phase 3 and Phase 4 (CLI synchronous path) are considered complete.
        *   **NEXT STEPS:**
            1.  **Proceed to Phase 5 (Frontend Basics)** - Focus on building the dashboard UI.
            2.  (Optional) Test asynchronous/Celery path if required.
            3.  (Optional) Consider adding unit tests for CLI commands.

### Phase 5: Frontend Basics (Management & Display)

*   âœ… **Create "Reports" Dashboard Section:** Add a new top-level section/route (e.g., `/reports`) in the Next.js dashboard.
    *   âœ… Basic dashboard page structure created
    *   âœ… Navigation routing implemented
    *   âœ… Layout with reports list and detail view areas
*   âœ… **List Reports:** Implement a UI list to display existing `Report`s.
    *   âœ… Successfully showing reports with timestamps and configuration info
    *   âœ… Correctly fetching the associated `Task` record to display status information
    *   âœ… Card-based layout consistent with other dashboards
    *   âœ… Removed redundant progress indicators and timestamps for cleaner UI
*   âœ… **Basic Report View:** Create a dedicated area to display a selected `Report`.
    *   âœ… Successfully showing report details when a report is selected
    *   âœ… Report cards display appropriate metadata
    *   âœ… Selection behavior working correctly
*   âœ… **List Configurations:** Implement a UI table/list to display existing `ReportConfiguration`s fetched via GraphQL.
    *   âœ… Created dedicated route at `/lab/reports/edit`
    *   âœ… Implemented grid view of report configurations
    *   âœ… Added create/edit/delete functionality
    *   âœ… Flat styling consistent with Plexus design system
*   âœ… **Basic Configuration Editor:** Create a simple form/modal to create/edit `ReportConfiguration`s.
    *   âœ… Created dedicated route at `/lab/reports/edit/[id]`
    *   âœ… Implemented Monaco editor for YAML configuration
    *   âœ… Added name and description fields
    *   âœ… Proper error handling and loading states
*   â¬œ **Trigger Generation from UI:** Add a button on the `ReportConfiguration` list/view to trigger a new report run **(invoking the Task creation/Celery dispatch mechanism from Phase 2)**.
*   âœ… **Fetch Report Data:** Implement logic on the report view page to fetch the `Report` record (including `output`), its associated `ReportBlock` records, **and the associated `Task` record (for status/metadata).** *(Fetching logic implemented, blocks are loaded)*
*   âœ… **Create Markdown Renderer:** Develop a component to render the report\'s Markdown content from `Report.output`. *(Basic rendering working in ReportTask)*
*   âœ… **Implement Block Reference System:** Create a system to identify and replace block references in the Markdown with corresponding block components. *(Initial version implemented via BlockRegistry and BlockRenderer)*
*   âœ… **Develop Block-Specific Components:** Create React components that render the JSON data from each `ReportBlock` type appropriately.
    *   âœ… `ScoreInfo` component created and successfully rendering block data in the UI.
*   âœ… **Verify Phase 5:** Confirm basic UI for listing, creating configurations, triggering runs, and viewing simple reports works. **Verify status display reflects the linked Task.** *(Core block rendering is functional)*

### Phase 6: Feedback Analysis Integration

*   **Objective:** Integrate feedback analysis as a core report type. This involves extracting historical feedback change data from the source system (Call Criteria DB) into standardized Plexus API models (`FeedbackItem`) and then implementing a Plexus report block to perform analysis (e.g., agreement scores like Gwet\'s AC1) directly on this Plexus data.
*   **Status (May, 2025):** Data extraction and API posting is complete. The `capture` command in `Call-Criteria-Python` successfully fetches change data, processes it, determines initial/final states, and upserts `FeedbackItem` records into the Plexus API via the `PlexusDashboardClient`. **Focus now shifts to implementing the analysis within Plexus.**

*   â¬œ **Refactor Feedback Analysis Code:**
    *   âœ… **Problem:** The current `analyze.py` file (1900+ lines) is too large for effective development and AI assistance sessions.
    *   âœ… **Approach:** Break the file into smaller, modular components:
        *   âœ… Create `utils.py` for shared utilities like `fetch_feedback_change_data`, `debug_log`, and database connection functions.
        *   âœ… Create `analyze_cmd.py` specifically for the `analyze` feedback command.
        *   â¬œ Create `trends_cmd.py` for the `trends` command functionality.
        *   âœ… Create `capture_cmd.py` for the `capture` command logic.
        *   â¬œ Refactor the CLI registration to import from these new modules.
        *   â¬œ Ensure all imports and dependencies are correctly managed in the new file structure.
        *   âœ… **Verification:** Thoroughly test each refactored command to ensure functionality is preserved. *(Verified `utils.py` and `analyze_cmd.py` split after troubleshooting import conflicts in `__init__.py` and `data.py`)*
    *   âœ… **Benefits:** Improved code maintainability, easier for AI sessions to process, better separation of concerns.

*   âœ… **Define Feedback Analysis Models:**
    *   âœ… Add `FeedbackItem` model in `resource.ts`.
    *   âœ… **(DONE)** ~~Add `FeedbackChangeDetail` model in `resource.ts`.~~ (Model removed)
    *   âœ… Define relationships.
    *   âœ… Define necessary secondary indexes.
*   âœ… **Refactor Data Extraction Logic in `analyze.py`:**
    *   âœ… Isolate Data Fetching Function (`fetch_feedback_change_data`).
    *   âœ… Refactor `analyze_feedback` to Use Fetch Function.
    *   âœ… Test `analyze` Command post-refactor.
*   âœ… **Implement `capture` CLI Command (Call-Criteria-Python):**
    *   âœ… Create `capture` command stub.
    *   âœ… Integrate Data Fetching (`fetch_feedback_change_data`).
    *   âœ… Display Raw Data Summary.
    *   âœ… Register with Main CLI.
    *   âœ… Test `capture` command fetching.
*   âœ… **Add Client-Side Models to `Plexus_2`:**
    *   âœ… Create Python model classes (`FeedbackItem`, `FeedbackChangeDetail`) in `Plexus_2/plexus/dashboard/api/models/`.
    *   âœ… Update `Plexus_2/plexus/dashboard/api/models/__init__.py`.
*   âœ… **Implement API Posting Logic in `capture` Command:**
    *   âœ… Initialize `PlexusDashboardClient`.
    *   âœ… Resolve `account_id`.
    *   âœ… Process Fetched Data (Grouping, Initial/Final State Derivation).
    *   âœ… Format Data for API (`FeedbackItem`, `FeedbackChangeDetail` payloads).
    *   âœ… Implement **Upsert Logic:** Query for existing records via `list()` and use `update()` or `create()` accordingly for both `FeedbackItem` and `FeedbackChangeDetail`.
    *   âœ… **Detailed Logging:** Use `rich.panel.Panel` with `rich.table.Table` for aligned key-value display of payloads being upserted. Include `None` values for comment fields.
*   âœ… **Implement `FeedbackAnalysisBlock` (Plexus Python):**
    *   âœ… Create new report block class `plexus.reports.blocks.FeedbackAnalysisBlock(BaseReportBlock)`.
    *   âœ… Implement `generate(config, params)` method.
    *   âœ… **Input Parameters:** Accept parameters like `scorecardId`, `dateRange` (start/end dates) via `params`.
    *   âœ… **API Querying:** Use `PlexusDashboardClient` within the block to query the Plexus API for relevant `FeedbackItem` records matching the input parameters (filtering by `accountId`, `scorecardId`, and potentially date range if timestamps are added to `FeedbackItem` or derived from `FeedbackChangeDetail`).
    *   âœ… **Analysis Logic:** Implement analysis logic (e.g., Gwet\'s AC1 calculation) using the fetched `FeedbackItem` data (comparing `initialAnswerValue` and `finalAnswerValue`). Group results by `scoreId` (Plexus Score ID).
    *   âœ… **JSON Output:** Return a JSON-serializable dictionary containing the analysis results (e.g., overall AC1, per-score AC1, mismatch counts, total items analyzed). This JSON will be stored in the corresponding `ReportBlock.output` field.
*   âœ… **Implement Frontend Component (`FeedbackAnalysis`):**
    *   âœ… Create a new React component (e.g., `FeedbackAnalysis.tsx`) specifically for rendering the output of the `FeedbackAnalysisBlock`.
    *   âœ… Register this component with the `BlockRegistry`.
    *   âœ… The component will receive the `output` JSON from the `ReportBlock` record as props.
    *   âœ… Render the analysis results effectively using individual score cards with gauges for AC1 and Accuracy, and a summary section.
*   âœ… **Add Testing:**
    *   âœ… Write unit/integration tests for `FeedbackAnalysisBlock`.
    *   âœ… Add Storybook stories for the `FeedbackAnalysis` component with various data scenarios (especially multiple scores if output changes).
    *   âœ… Consider integration tests to verify the component correctly displays data from the API.
*   âœ… **Verify Phase 6:** Confirm the `FeedbackAnalysisBlock` correctly queries the Plexus API, performs the analysis, stores results, and the frontend component renders the results accurately within a generated report.

ðŸŸ¡ **Next Steps for FeedbackAnalysis:**
- â¬œ **Enhance `capture` CLI Command for Multi-Score Support (Call-Criteria-Python):** (See detailed plan in Phase 6)
- â¬œ **Enhance `FeedbackAnalysisBlock` (Python Backend):**
  - â¬œ Modify the Python `FeedbackAnalysisBlock` to accept a list of score IDs/names as input (e.g., via `params` in the report configuration).
  - â¬œ Update the block's `generate` method to fetch `FeedbackItem` data for all specified scores (potentially across multiple scorecards if the design evolves, or for multiple scores within the primary scorecard context defined for the block).
  - â¬œ Aggregate data appropriately (e.g., calculate overall AC1 across all included scores, and provide per-score breakdowns).
  - â¬œ Ensure the JSON output structure can clearly represent both overall and per-score results.
- â¬œ **Add Testing:**
  - â¬œ Add Storybook stories for the `FeedbackAnalysis` component to test rendering with data from multiple scores.
  - â¬œ Consider integration tests to verify the component correctly displays data from the API with multi-score results.
  - â¬œ Write unit/integration tests for the Python `FeedbackAnalysisBlock`, including multi-score scenarios and dynamic ID resolution in the `capture` command.

### Block Rendering Implementation Updates

âœ… **Fixed Critical Layout Issue with Code Blocks:**
- Successfully resolved the layout issues with preformatted code blocks
- Implemented proper text wrapping and horizontal scrolling without breaking the containing layout
- Used a combination of CSS properties to ensure blocks don\'t expand beyond their container:
  - `min-w-0` to allow flex items to shrink below their content size
  - `max-w-full` to prevent overflow
  - `whitespace-pre-wrap` and `break-all` for text wrapping
  - `overflow-x-auto` for horizontal scrolling when needed

âœ… **Implemented Component Structure for Report Blocks:**
- Created minimalist `ReportBlock` component (default fallback)
- Implemented `BlockRegistry` and `BlockRenderer` system for registering and retrieving specialized block components.
- `BlockRenderer` now wraps blocks in a standard container (`rounded-lg bg-background p-4 my-4`).
- Resolved circular dependency issues with registration by creating `registrySetup.ts` and importing it in both the dashboard and Storybook preview.
- `BlockRenderer` now gracefully handles unknown block types by rendering the default `ReportBlock` with an error message title.
- Established proper component hierarchy and interfaces
- Set up efficient data flow from report to blocks

âœ… **Created First Specialized Block (`ScoreInfo`):**
- Implemented `ScoreInfo` component for score information display.
- Added colorful badges for visualizing metrics (accuracy, value).
- Properly formatted dates and percentages.
- Used Shadcn UI components for consistent styling.
- Named component to match Python class name for consistency.
- **Successfully integrated and displayed `ScoreInfo` component within the main Reports Dashboard UI, rendering data from fetched `ReportBlock` records.**

âœ… **Added Storybook Stories for Block Development:**
- Created dedicated block stories directory.
- Updated stories to use `BlockRenderer` as the main component.
- Added decorator for `bg-card` background to improve visibility of blocks.
- Added comprehensive story examples with various data scenarios:
  - Basic rendering
  - Long content handling
  - Very long JSON content
  - Blocks within a ReportTask context
  - Performance levels (good/medium/poor metrics)
  - Missing data handling
  - Unknown block type handling (renders default block with error title).

