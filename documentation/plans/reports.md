# Plexus Reports Feature Plan

**Status Legend:**
*   â¬œ Not Started / To Do
*   ðŸŸ¡ In Progress
*   âœ… Completed

***Note on Testing:*** *Test files in this project are typically located directly adjacent to the source code file they are testing (e.g., `service.py` would have a corresponding `service_test.py` in the same directory).*

---
**CRITICAL INSTRUCTIONS FOR AI ASSISTANTS WORKING ON THIS FEATURE:**

1.  **DO NOT RUN `plexus` COMMANDS DIRECTLY.** This project often exists alongside other clones. The globally installed `plexus` command likely points to a different repository. **ALWAYS** run CLI commands using the local Python module structure from the project root:
    ```bash
    python -m plexus.cli.CommandLineInterface [command] [args...]
    ```
    This ensures you are executing the code currently being worked on.

2.  **NEVER ASSUME A COMMAND WORKED BECAUSE IT PRODUCED NO OUTPUT.** This is a dangerous and incorrect assumption. Terminal commands, especially in this project, may succeed silently or fail silently (or with errors that are missed if you don't wait). **YOU MUST ALWAYS WAIT FOR THE COMMAND TO COMPLETE AND CAREFULLY ANALYZE ITS ACTUAL OUTPUT AND EXIT CODE.** If a command appears to produce no output, verify its success through subsequent commands (e.g., listing created items, checking status) before proceeding. Failure to do this will lead to incorrect actions and wasted effort.
---

## Introduction

This document outlines the plan for implementing a flexible and extensible reporting system within the Plexus platform. The goal is to provide a standardized way to define, generate, store, and view various types of reports and analyses without requiring bespoke dashboard pages or API schema changes for each new report type. This system will support reports like feedback analysis, topic modeling, score performance summaries, and more.

**Crucially, report generation will leverage the existing `Task` and `TaskStage` system for standardized status and progress tracking, ensuring consistency with other background jobs like Evaluations.**

**Note on Running CLI Commands:** When working within this specific project checkout (maybe `Plexus`, maybe `Plexus_2`, there are multiple clones), run CLI commands using `python -m plexus.cli.CommandLineInterface [command] [args...]` from the project root directory. This ensures the local code is executed without interfering with the globally installed `plexus` module from a different repository.

## Core Concepts

The reporting system will be built around **four** core concepts:

*   **`ReportConfiguration`**: Defines the structure, content sources, and parameters for a specific type of report using Markdown and Jinja2 templating. It acts as a template for generating reports and specifies which `ReportBlock` Python classes to execute.
*   **`Report`**: Represents a specific instance of a report generated based on a `ReportConfiguration`. It stores the final rendered output (e.g., Markdown) in its `output` field and links to the individual `ReportBlock` results.
*   **`ReportBlock` (Database Model)**: Stores the structured JSON output and optional logs generated by a specific Python `ReportBlock` execution within a `Report` run. Each block has a defined `position` and an optional `name`.
*   **`ReportBlock` (Python Class)**: Reusable Python components responsible for generating specific sections or data points within a report. These blocks encapsulate the logic for fetching data and performing analysis, returning JSON data to be stored in the `ReportBlock` database model.
*   **`Task` / `TaskStage`:** The standard mechanism for dispatching, monitoring, and tracking the progress of the report generation job itself. Each `Report` instance will be directly linked to a corresponding `Task` record.

## Data Models

### `ReportConfiguration`

*   **Storage:** Likely stored as YAML or JSON within a new database model (e.g., `ReportConfiguration`). This allows for versioning and easy editing.
*   **Structure:**
    *   `name`: Human-readable name for the configuration.
    *   `description`: Optional description.
    *   `accountId`: Link to the owning account.
    *   `configuration`: The core YAML/JSON definition. This would specify:
        *   Static content (headers, paragraphs, images).
        *   Report Blocks to include, along with their specific parameters (e.g., `scorecardId`, `timeRange`, `pythonClass`).
        *   Layout or ordering information for the blocks.
    *   Standard metadata (`createdAt`, `updatedAt`, etc.).
*   **API Access:** Use `plexus.dashboard.api.models.report_configuration.ReportConfiguration` model class.

### `Report`

*   **Storage:** A new database model (`Report`) linked to a `ReportConfiguration` and a `Task`.
*   **Structure:**
    *   `reportConfigurationId`: Link to the configuration used.
    *   `accountId`: Link to the owning account.
    *   `name`: Can be auto-generated or user-defined.
    *   `taskId`: **Required** link to the associated `Task` record that handles the generation process and status tracking.
    *   `createdAt`, `updatedAt`: Standard metadata.
    *   `parameters`: Parameters used for this specific run (might override or supplement configuration).
    *   `output`: The final rendered report output, stored as a string (e.g., Markdown). Generated by processing the `ReportConfiguration.configuration` template.
    *   `reportBlocks`: A one-to-many relationship linking to the individual `ReportBlock` results generated for this report.
    *   `shareLinks`: Association for shareable URLs.
    *   **(Removed Fields):** `status`, `startedAt`, `completedAt`, `errorMessage`, `errorDetails` are **removed** from this model. This information is now managed by the associated `Task` record.
*   **API Access:** Use `plexus.dashboard.api.models.report.Report` model class.

### `ReportBlock`

*   **Storage:** A new database model (`ReportBlock`) linked to a `Report`.
*   **Structure:**
    *   `reportId`: Link to the parent `Report`.
    *   `name`: Optional user-defined name for the block (extracted from the block definition in the configuration).
    *   `position`: Required integer indicating the order/position of the block within the report configuration.
    *   `output`: The structured data generated by the corresponding Python `ReportBlock` class, stored as JSON.
    *   `log`: Optional string containing logs or messages from the block's execution.
    *   Standard metadata (`createdAt`, `updatedAt`).
*   **Indexes:**
    *   `byReportAndName`: GSI to query blocks by `reportId` and `name`.
    *   `byReportAndPosition`: GSI to query blocks by `reportId` and `position`.
*   **API Access:** Use `plexus.dashboard.api.models.report_block.ReportBlock` model class.

## Backend Implementation

*(Note: The API client is available via `plexus.dashboard.api.client.PlexusDashboardClient`)*

### Python `ReportBlock` Framework

*   Define a base Python class (e.g., `plexus.reports.blocks.BaseReportBlock`).
*   Subclasses will implement specific report generation logic (e.g., `FeedbackAnalysisBlock`, `TopicModelBlock`, `ScorePerformanceBlock`).
*   Blocks will implement a standard method (e.g., `generate(config, params)`) that returns a JSON-serializable dictionary (stored in `ReportBlock.output`) and optionally a log string (stored in `ReportBlock.log`).
*   Blocks should have access to Plexus data fetching utilities (e.g., to query `Score`, `Evaluation`, `Item` data via the API or direct DB access if necessary).

### Report Generation Service

*   A mechanism to trigger report generation based on a `ReportConfiguration`, **which will always create and dispatch a `Task`**. The service logic itself will be executed via a Celery task triggered by this `Task` record.
*   **Invocation:** The service will be invoked with a `task_id`.
*   The service, running within the Celery task context, will:
    1.  Fetch the associated `Task` record using the `task_id`.
    2.  Load the `ReportConfiguration` (referenced by the Task or Report) and run `parameters` (likely stored in the Task or Report).
    3.  **Update Task Status:** Set the `Task` status to `RUNNING` and initialize progress tracking (e.g., using `TaskProgressTracker`).
    4.  **Create Report Record:** Create the `Report` database record, linking it to the `Task` (`taskId`) and `ReportConfiguration`.
    5.  Parse the `ReportConfiguration.configuration` Markdown to extract block definitions and the original Markdown template.
    6.  **Process Blocks:** For each extracted block definition: Instantiate and call the `generate` method. Create `ReportBlock` records storing the results. **Update Task/Stage progress via `TaskProgressTracker` as blocks complete.**
    7.  **Store Original Markdown:** Store the reconstructed, original Markdown string (from step 5) into the `Report.output` field.
    8.  **Update Final Task Status:** Upon successful completion or failure, update the associated `Task` record's final `status`, `completedAt`, `errorMessage`, `errorDetails` etc. **The `Report` record itself is not directly updated with status information.**

### Synchronous vs. Asynchronous Execution (Clarification)

Initial testing and discussion revealed the need for two distinct report generation execution paths, while ensuring core logic reuse and consistent progress tracking:

1.  **Synchronous Execution (CLI):**
    *   The `plexus report run` CLI command **MUST** execute the report generation process *synchronously* within the same process.
    *   It **MUST** create a `Task` record to represent the job and use `TaskProgressTracker` to manage status and stages.
    *   It **MUST NOT** dispatch the job to Celery. It should directly invoke the core report generation logic.
    *   The CLI command will wait for the generation to complete and report the final `Task` status and the resulting `Report` ID.
    *   This path is primarily intended for direct user interaction, testing, and scenarios where immediate feedback is desired without relying on background workers.

2.  **Asynchronous Execution (via Celery Worker):** The core report generation logic is executed by a standard Celery worker (`plexus command worker`) processing the `generate_report_task`. This task can be dispatched in two primary ways:
    *   **a) Direct Celery Dispatch:** A user or script can directly dispatch the `generate_report_task` using `plexus command dispatch [task_id] [other_args...]`. The worker receives this directly via the Celery queue.
    *   **b) API/Lambda Trigger:**
        *   A `Task` record is created via the GraphQL API (e.g., initiated by the dashboard) with metadata indicating a report generation request (including the `report_configuration_id`).
        *   A separate process (e.g., an AWS Lambda function triggered by DynamoDB stream events on the `Task` table) detects the creation of this new `Task` record configured for report generation.
        *   This trigger process then dispatches the actual `generate_report_task` to the Celery queue, passing the `task_id`.
        *   A Celery worker then picks up this dispatched task and executes the generation.
    *   In both asynchronous scenarios, the Celery task handler (the Python function decorated with `@celery.task`, likely calling `generate_report(task_id)`) uses the `task_id` to initialize `TaskProgressTracker` and invokes the *same core report generation logic* as the synchronous path. This ensures consistency regardless of how the job was initiated.

**Implementation Note:** This requires refactoring `plexus.reports.service` to:

1.  **Isolate Core Logic:** Create a new private function, `_generate_report_core`, that encapsulates the primary report generation steps (loading config, creating `Report` record, parsing markdown, running blocks, creating `ReportBlock` records, updating progress, setting final status).
2.  **Define `_generate_report_core` Parameters:** This function will accept direct inputs:
    *   `report_config_id: str`
    *   `account_id: str`
    *   `run_parameters: dict`
    *   `client: PlexusDashboardClient`
    *   `tracker: TaskProgressTracker` (This is crucial for linking to the `Task` and updating progress/status).
    *   It should return the ID of the created `Report` record upon success, or raise an exception on failure (allowing the caller to handle final Task status updates via the tracker).
3.  **Adapt `generate_report(task_id)`:** Modify the existing function (used by Celery workers):
    *   It will retain its `task_id: str` signature.
    *   Fetch the `Task` record using the `task_id`.
    *   Extract `report_config_id`, `account_id`, and `run_parameters` from `Task.metadata`.
    *   Initialize the `TaskProgressTracker` using the `task_id`.
    *   Wrap the call to `_generate_report_core` in a `try...except` block.
    *   Call `_generate_report_core` with the extracted parameters and the tracker.
    *   On success, ensure the tracker marks the task as `COMPLETED`.
    *   On exception during the `_generate_report_core` call, use the tracker to mark the task as `FAILED` with the error details.
4.  **Adapt `plexus report run` CLI (Next Step):** This CLI command will be modified *after* the service refactoring to:
    *   Create a new `Task` record.
    *   Initialize `TaskProgressTracker`.
    *   Directly call `_generate_report_core` synchronously, passing the necessary parameters and the tracker.
    *   It will *not* involve Celery dispatch.

This refactoring ensures the core report generation logic is DRY and consistently uses the `TaskProgressTracker`, while supporting both synchronous CLI execution and asynchronous Celery-based execution.

## Frontend Implementation (Dashboard)

### Management Interface

*   New dashboard section for "Reports".
*   View/List existing `ReportConfiguration`s and `Report`s.
*   Create/Edit `ReportConfiguration`s:
    *   Potentially a YAML/JSON editor.
    *   A more user-friendly UI builder could be a future enhancement.
*   Trigger new `Report` runs from a configuration.

### Report Viewing

*   Dedicated page or component to display a `Report`.
*   Fetch the `Report` record, including its `output` string and its associated `ReportBlock` records (sorted by `position`).
*   Render the `Report.output` string, likely using a Markdown renderer component.
*   Display the data from the associated `ReportBlock` records. This could involve:
    *   A separate section/tab listing each block (by `name` or `position`).
    *   Dynamically rendering the `output` JSON from each `ReportBlock` using appropriate React components (tables, charts, key metrics, text sections) based on the JSON structure or hints within it.
*   **Sharing:** Integrate with the existing `ShareLink` system to allow sharing report URLs.
*   **Printing:** Implement CSS media queries (`@media print`) to provide a clean, printable version of the report view, removing UI chrome.

## Implementation Plan & Checklist

*   âœ… **Define Models:** Define `ReportConfiguration`, `Report`, and `ReportBlock` models in `dashboard/amplify/data/resource.ts`.
    *   âœ… Add fields for `ReportConfiguration` (name, description, accountId, configuration (json), createdAt, updatedAt).
    *   âœ… **Modify `Report`:** Add required `taskId` field. **Remove** `status`, `startedAt`, `completedAt`, `errorMessage`, `errorDetails` fields.
    *   âœ… Add fields for `ReportBlock` (reportId, name, position, output (json), log, createdAt, updatedAt).
*   âœ… **Define Relationships:**
    *   âœ… Add necessary relationships (`Account` -> `ReportConfiguration`, `ReportConfiguration` -> `Report`, `Account` -> `Report`).
    *   âœ… **Add `Task <-> Report` relationship:** Add `report: hasOne` to `Task` and `task: belongsTo` (linked via required `taskId`) to `Report`.
    *   âœ… Add (`Report` -> `ReportBlock`).
*   âœ… **Add Indexes:**
    *   âœ… Define required secondary indexes (`ReportConfiguration` by accountId/updatedAt, name; `ReportBlock` by reportId/name, reportId/position).
    *   âœ… **Modify `Report` Indexes:** Remove index on `status`. Add index on `taskId`.

### Phase 1: Backend Foundation (Post-Schema)

*   âœ… **Create Base Python Class:** Create the base `plexus.reports.blocks.BaseReportBlock` Python class (`plexus/reports/blocks/base.py`) with a placeholder `generate` method.
*   âœ… **Verify Phase 1:** Confirm models exist in the backend and that the auto-generated base GraphQL CRUD operations (e.g., `getReport`, `listReports`, `createReportConfiguration`) work as expected via AppSync console or tests.

### Phase 2: Report Generation (Service & Triggering)

*   âœ… **Use Existing Test Block:** Use the existing `ScoreInfo` block (in `plexus/reports/blocks/score_info.py`) for initial testing instead of creating a separate `HelloWorld` block. *(Renamed from ScoreInfoBlock)*
*   âœ… **Develop Generation Service Core:** Create Python service logic (`plexus.reports.service`) that:
    *   âœ… Takes a `task_id` as input.
    *   âœ… Fetches the associated `Task` and loads `ReportConfiguration` and parameters.
    *   âœ… **Integrate Task Progress:** Use `TaskProgressTracker` to update `Task` and `TaskStage` status (e.g., `RUNNING`, progress updates during block processing, `COMPLETED`/`FAILED`).
    *   âœ… Creates the `Report` record linked to the `Task`.
    *   âœ… Parses the `configuration` field (Markdown) to identify block definitions.
    *   âœ… Processes Blocks: Instantiates and calls `generate` for each block. Creates `ReportBlock` records.
    *   âœ… Stores the original Markdown template in `Report.output`.
*   âœ… **Implement CLI Trigger:** Create the `plexus report run --config <config_identifier> [params...]` CLI command that:
    *   âœ… Parses arguments.
    *   âœ… **Creates a `Task` record** for the report generation.
    *   âœ… **Initializes `TaskProgressTracker`** using the created `task_id`.
    *   âœ… **Directly calls the core generation logic** (e.g., `_generate_report_core`) synchronously.
    *   âœ… **Waits for completion** and reports final `Task` status and `Report` ID.
    *   âœ… **Does NOT dispatch to Celery.**
*   âœ… **(Removed) Basic Status Updates:** Status updates are now handled via the `Task` model and `TaskProgressTracker`.
*   âœ… **Implement Celery Task (`generate_report_task`):**
    *   âœ… Takes `task_id`.
    *   âœ… Calls the `plexus.reports.service.generate_report` service function, passing the `task_id`. (This function now wraps `_generate_report_core` and handles task loading/tracker init).
    *   âœ… **Handles top-level exceptions:** Catches errors from the service call and updates the corresponding `Task` record to `FAILED` with error details.
*   âœ… **(Removed/Clarified) Implement Celery Dispatch Mechanism:** Celery dispatch is now primarily handled by API/Lambda triggers based on `Task` creation, or manually via `plexus command dispatch`, *not* by the `plexus report run` command itself.
*   ðŸŸ¡ **Verify Phase 2:** Confirm reports can be generated via CLI/Celery, data is stored correctly in `Report` and `ReportBlock`, and **Task/TaskStage status updates correctly**.
    *   *Status:* We have successfully created test `ReportConfiguration`s via the CLI (`create-config` and `report config create --file ...`). We have also successfully triggered generation using `report run`, which created Task `4b688330-704e-485c-b7ca-8e0d95a16346`. The task is currently `PENDING`/`QUEUED`.
    *   ***NEXT:*** *Start a Celery worker (`python -m plexus.cli.CommandLineInterface command worker`) and observe its logs to confirm it processes Task `4b688330-704e-485c-b7ca-8e0d95a16346` and updates its status/stages correctly. Check final status using `python -m plexus.cli.CommandLineInterface tasks info --id 4b688330-704e-485c-b7ca-8e0d95a16346`.*

### Phase 3: CLI Inspection Tools (Pre-UI Validation)

*   **CLI Output Style:** For consistency, prefer using `rich.panel.Panel` with `expand=True` for displaying multi-line details in CLI command outputs.
*   **Note on ID/Name Lookup:** Commands accepting `<id_or_name>` should intelligently attempt lookup: Check if input looks like a UUID. If yes, try ID first, then name. If no, try name first, then ID. Always try both before failing.
*   âœ… **Implement `plexus report config list`:** Create a CLI command to list `ReportConfiguration` records. Use `rich` for formatted table output. *(Verified)*
*   âœ… **Implement `plexus report config show <id_or_name>`:** Create a CLI command to display details of a specific `ReportConfiguration`. Use `rich` panels/syntax highlighting. Implement ID/Name lookup.
*   âœ… **Implement `plexus report list [--config <id_or_name>]`:** Create a CLI command to list `Report` records, including linked `taskId` and basic `Task` status. Use `rich` table output. Support optional filtering by `ReportConfiguration` (implementing ID/Name lookup for the filter value).
*   âœ… **Implement `plexus report show <id_or_name>`:** Create a CLI command to display details of a specific `Report`, including parameters, output (rendered Markdown if possible), and associated `ReportBlock` summaries. Use `rich` panels. Implement ID/Name lookup.
*   âœ… **Implement `plexus report last`:** Create a CLI command to show the details of the most recently created `Report` (equivalent to `plexus report show` for the latest report). Use `rich` panels.
*   ðŸŸ¡ **Verify Phase 3:** Confirm these CLI commands function correctly, including filtering and ID/Name lookup, providing the necessary visibility into report data. *(Verified config list, config show, config create, config delete manually. Report and Block commands remain.)*

### Phase 4: Backend & CLI Testing

*   ðŸŸ¡ **Objective:** Verify the end-to-end functionality of report configuration management, generation triggering via CLI, Celery task execution, data storage, status tracking, and CLI inspection tools.
*   â¬œ **Prerequisites:**
    *   Ensure a Celery worker can be started (`python -m plexus.cli.CommandLineInterface command worker`).
    *   Ensure necessary environment variables are set (e.g., `PLEXUS_ACCOUNT_KEY` in `.env`) and **loaded by the application**.
    *   Ensure the database schema is up-to-date.
    *   â¬œ **Create Test Config File:** Create a file named `test_config.md` with sample Markdown content, e.g.:
        ```markdown
        # Test Report Header

        This is a sample configuration.

        ```block name="Score Info Block"
        class: ScoreInfo
        scorecard: cmg_edu_v1_0
        score: Greeting
        ```
        ```
*   â¬œ **Test Steps:**
    1.  âœ… **`config list`:**
        *   Run `python -m plexus.cli.CommandLineInterface report config list`. Verify existing configs are listed correctly for the default account (resolved via `.env`).
    2.  âœ… **`config create`:** *(Manually Tested - See notes below)*
        *   âœ… Run `python -m plexus.cli.CommandLineInterface report config create --name \"CLI Test Config\" --file test_config.md`. Verify successful creation message (`Successfully created Report Configuration...`) and that the config appears in `config list` output. *(Verified manually)*
        *   âœ… **Update (2025-04-28):** Fixed `test_config.md` format (name inside YAML block). Re-ran create, **currently paused awaiting confirmation [y/N] to overwrite.**
        *   â¬œ Attempt creation with missing required options (e.g., `--name` or `--file`). Verify Click error. (`python -m plexus.cli.CommandLineInterface report config create --name \"Missing File\"`)
        *   â¬œ Attempt creation with a non-existent file path. Verify error message. (`python -m plexus.cli.CommandLineInterface report config create --name \"Bad File Path\" --file non_existent_file.md`)
    3.  âœ… **`config delete`:** *(Manually Tested - See notes below)*
        *   âœ… **Delete by Name (with prompt):** Run `python -m plexus.cli.CommandLineInterface report config delete \"CLI Test Config\"`. Verify:
            *   It finds the correct config (shows ID/Name).
            *   It prompts for confirmation (`Are you sure...?`).
            *   Respond 'y'. Verify success message.
            *   Run `config list` again and verify \"CLI Test Config\" is gone. *(Verified manually - Required multiple fixes 2025-04-28)*
        *   âœ… **Delete multiple by name (with prompt):** Run `python -m plexus.cli.CommandLineInterface report config delete \"Example From File CLI\"`. Verify it prompts for and successfully deletes multiple matching entries. *(Verified 2025-04-28)*
        *   âœ… **Delete multiple by name (skip prompt):** Run `python -m plexus.cli.CommandLineInterface report config delete \"Example From File CLI\" --yes`. *(Not directly tested with multiple, but tested on single entries below)*
        *   âœ… **Delete by Name (skip prompt):** Run `python -m plexus.cli.CommandLineInterface report config delete \"CLI Test Config\" --yes`. Verify:
            *   It finds the correct config.
            *   It prints the \"Skipping confirmation\" message. *(Verified 2025-04-28)*
            *   It prints the success message. *(Verified 2025-04-28)*
            *   Run `config list` again and verify the config is gone. *(Verified 2025-04-28)*
        *   âœ… **Delete Non-Existent:** Run `python -m plexus.cli.CommandLineInterface report config delete \"NonExistent Config\"`. Verify \"not found\" message. *(Verified 2025-04-28)*
        *   âœ… **Recreate final test config:** Run `python -m plexus.cli.CommandLineInterface report config create --name \"CLI Test Config\" --file test_config.md`. *(Verified 2025-04-28)*
    5.  âœ… **`report run` (Core Test):**
        *   Run `python -m plexus.cli.CommandLineInterface report run --config \"CLI Test Config\"`. Verify:
            *   Task creation message with Task ID is shown.
            *   Messages indicating synchronous execution and progress updates are shown (e.g., block processing logs, final status).
            *   **No Celery dispatch message is shown.**
            *   The command waits for completion before returning the prompt.
            *   Final `Task` status and `Report` ID are printed upon completion.
            *   âœ… **Verified (2025-04-28):** Command runs successfully after multiple fixes (missing methods, tracker errors, GraphQL issues, block parsing, async issues, model instantiation).
        *   â¬œ Run `python -m plexus.cli.CommandLineInterface report run --config <invalid_config_id_or_name>`. Verify error message (config resolution failure).
        *   â¬œ Run `python -m plexus.cli.CommandLineInterface report run --config \"CLI Test Config\" invalid_param=test`. Verify parameter parsing error or successful run with parameters logged in Task metadata.
        *   **(Removed) Monitor Celery Worker:** Worker monitoring is not needed for synchronous CLI execution testing. (Worker testing should happen separately for async paths).
        *   âœ… **Check Task Status:** Use `python -m plexus.cli.CommandLineInterface task get --id <task_id_from_run>` to check final status (`COMPLETED` or `FAILED`), completion time, error messages (if any). Verify this matches the status reported by the `report run` command directly. *(Verified status matches)*
        *   âœ… **Check Database:** (Optional) Manually inspect `Report` and `ReportBlock` tables to confirm data persistence. *(Verified Report and ReportBlock created)*
    6.  ðŸŸ¡ **`report list`:**
        *   Run `python -m plexus.cli.CommandLineInterface report list`. Verify the newly generated report appears with correct Config ID, Task ID, and fetched Task Status.
        *   Run `python -m plexus.cli.CommandLineInterface report list --config \"CLI Test Config\"`. Verify filtering works (using ID/Name lookup for the config).
        *   Run `python -m plexus.cli.CommandLineInterface report list --config \"NonExistent Config\"`. Verify appropriate \"not found\" or empty message.
        *   âœ… **Verified (2025-04-28):** Command works after fixing list handling in CLI command code.
        *   ðŸŸ¡ **Status:** Currently broken - Rich table rendering fails. Task status fetching was also commented out during debug.
    7.  âœ… **`report show`:**
        *   Identify the ID or Name of the report generated in step 4.
        *   Run `python -m plexus.cli.CommandLineInterface report show <report_id_or_name>`. Verify:
            *   Correct Report details are shown.
            *   Associated Task details (Status, Timestamps, Errors, Metadata) are fetched and displayed.
            *   Report Output (Markdown template) is displayed.
            *   Associated Report Blocks summary table is shown.
            *   ID/Name lookup works for the report itself.
            *   âœ… **Verified (2025-04-28):** Command works after fixing list handling for blocks.
        *   Run `python -m plexus.cli.CommandLineInterface report show <non_existent_report_id>`. Verify \"not found\" message.
    8.  âœ… **`report last`:**
        *   Run `python -m plexus.cli.CommandLineInterface report last`. Verify it finds the most recently generated report (from step 4) and displays the same details as `report show` for that report.
        *   âœ… **Verified (2025-04-28):** Command works after fixing sort parameter and list handling.
    9.  ðŸŸ¡ **`block list`:**
        *   Get the `report_id` from step 4.
        *   Run `python -m plexus.cli.CommandLineInterface report block list <report_id>`. Verify the blocks created by the `ScoreInfo` class appear with correct position, name (if any), output summary, and log status.
        *   Run `python -m plexus.cli.CommandLineInterface report block list <invalid_report_id>`. Verify \"not found\" or empty message.
        *   ðŸŸ¡ **Status:** Currently broken - Fails due to direct list return from `list_by_report_id`.
    10. âœ… **`block show`:**
        *   Get the `report_id` and a `block_identifier` (position \'0\' or name if defined by the block) from the previous step.
        *   Run `python -m plexus.cli.CommandLineInterface report block show <report_id> <block_identifier>`. Verify:
            *   Correct block details (Position, Name, Timestamps) are shown.
            *   Output JSON is fetched, parsed, and displayed with syntax highlighting.
            *   Log content is fetched and displayed.
            *   Lookup works by both position (e.g., \'0\') and name (if applicable).
            *   âœ… **Verified (2025-04-28):** Command works after fixing list handling.
        *   Run `python -m plexus.cli.CommandLineInterface report block show <report_id> <invalid_identifier>`. Verify \"not found\" message.
*   ðŸŸ¡ **Verify Phase 4:** Confirm all test steps pass, demonstrating reliable backend processing and CLI interaction for the reports feature.
    *   **Notes (2025-04-28 Session End):**
        *   Successfully tested `report config` commands (list, show, create).
        *   Fixed and successfully tested `report config delete`, including handling multiple matches by name and the `--yes` flag. Cleaned up test configurations. The CLI output buffering/interleaving issue seems resolved now that internal errors in `delete_config` are fixed.
        *   Successfully tested `report run` (sync) after fixing several Python client model issues (missing methods, wrong args, list handling, GraphQL naming/types), TaskProgressTracker issues (task ID access, stage name sanitization), service logic (block parsing, async/await, JSON serialization), and model instantiation (`from_dict`). Core synchronous generation works.
        *   Successfully tested `report show` and `report last`.
        *   Successfully tested `report block show`.
        *   Commands `report list` and `report block list` are currently broken due to table rendering/list handling issues.
        *   **CURRENT STATUS:** Phase 4 testing mostly complete. Core `report run` works. `show` commands work. `list` commands are broken.
        *   **NEXT STEPS (Next Session):**
            1.  Fix `report list` command:
                *   Replace `rich.Table` with `rich.Panel` output, similar to `config list` or `report show`.
                *   Default to showing only the 10 most recent reports (use `Report.list_by_account_id` sorting/limiting if possible, otherwise client-side).
                *   Re-enable task status fetching and display within the panel output.
            2.  Fix `report block list` (likely involves same direct list handling fix as applied to `show_block`).
            3.  Test error handling for `report run` (e.g., invalid config name, bad parameters).
            4.  Consider adding unit tests for the CLI commands.
    *   **Notes (2025-04-27):** *(Previous notes kept for history)*
        *   Manually tested `report config create` with `--description` and duplicate name scenarios (different/identical content prompts).
        *   Findings:\n            *   Creation with description works.\n            *   Duplicate check (different content) prompt/abort/proceed works as expected.\n            *   Duplicate check (identical content) logic is present and passes unit tests, but the current `ReportConfiguration.get_by_name` implementation prevents it from being triggered correctly when multiple configs share the same name (it only compares against one existing config).\n        *   Manually tested `report config delete`.\n        *   Findings:\n            *   Deletion by name works (including handling multiple matches sequentially).\n            *   The `--yes` flag does not correctly suppress the confirmation prompt and needs fixing.\n        *   **TODO:** Refine `ReportConfiguration.get_by_name` or the duplicate check logic in `create_config` to handle multiple existing configurations with the same name correctly (currently only compares against one match).\n        *   **TODO:** Fix `--yes` flag in `report config delete` command to correctly bypass the confirmation prompt.\n

### Phase 5: Frontend Basics (Management & Display)

*   â¬œ **Create "Reports" Dashboard Section:** Add a new top-level section/route (e.g., `/reports`) in the Next.js dashboard.
*   â¬œ **List Configurations:** Implement a UI table/list to display existing `ReportConfiguration`s fetched via GraphQL.
*   â¬œ **List Reports:** Implement a UI table/list to display existing `Report`s. **Fetch the associated `Task` record to display the generation status.**
*   â¬œ **Basic Configuration Editor:** Create a simple form/modal to create/edit `ReportConfiguration`s.
*   â¬œ **Trigger Generation from UI:** Add a button on the `ReportConfiguration` list/view to trigger a new report run **(invoking the Task creation/Celery dispatch mechanism from Phase 2)**.
*   â¬œ **Basic Report View:** Create a dedicated route/page (e.g., `/reports/[reportId]`).
*   â¬œ **Fetch Report Data:** Implement logic on the report view page to fetch the `Report` record (including `output`), its associated `ReportBlock` records, **and the associated `Task` record (for status/metadata).**
*   â¬œ **Initial Dynamic Rendering:** Develop a Markdown renderer for `Report.output`. Implement basic display for `ReportBlock` data. **Display generation status/errors fetched from the linked `Task`.**
*   â¬œ **Verify Phase 5:** Confirm basic UI for listing, creating configurations, triggering runs, and viewing simple reports works. **Verify status display reflects the linked Task.**

### Phase 6: Advanced Features & Polish

*   â¬œ **Implement Core Report Blocks:**
    *   â¬œ Implement `FeedbackAnalysisBlock`.
    *   â¬œ Implement `ScorePerformanceBlock`.
    *   â¬œ Implement `TopicModelBlock` (if applicable).
*   â¬œ **Develop Corresponding React Components:** Create specific React components to visualize the data from `ReportBlock.output` JSON (e.g., charts for performance, tables for feedback, topic lists).
*   â¬œ **Enhance Dynamic Rendering:** Improve the report viewing component to intelligently select and render the appropriate React component based on the structure/type information within the `ReportBlock.output` JSON.
*   â¬œ **Integrate Sharing:** Connect the `Report` model to the `ShareLink` system.
*   â¬œ **Improve Configuration Editor:** Consider a more user-friendly UI beyond raw YAML/JSON/Markdown editing (future enhancement).
*   â¬œ **Refine Print Styles:** Ensure the `@media print` styles produce a high-quality printed report, handling block rendering appropriately.
*   â¬œ **Verify Phase 6:** Test complex reports with various blocks, ensure proper visualization, sharing, and printing.

## Example Report Configuration

Here is an example of the content stored in the `ReportConfiguration.configuration` field:

```block name="Term Life - Temperature Check - Score Information"
class: ScoreInformation
scorecard: termlifev1
score: Temperature Check
```

The parameters for blocks work in the same way as parameters to the CLI tools: `scorecard` can be an ID, an external ID, a key, or a name. Same for `score`.

When the report is generated, each report block in the report will generate structured JSON output that will be stored in a separate `ReportBlock` entry, linked to the `Report`. The JSON output for each block goes into `ReportBlock.output`. The block's execution logs can be stored in `ReportBlock.log`.

*(Note: The example above shows `ScoreInformation`. We should ensure consistency with the actual class name, which we intend to be `ScoreInfo`.)*

